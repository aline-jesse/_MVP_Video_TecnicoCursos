/**
 * üé≠ AVATAR 3D RENDERING SERVICE
 * Servi√ßo de renderiza√ß√£o de Avatar 3D com sincroniza√ß√£o labial
 */

import { NextRequest, NextResponse } from 'next/server'
import { getServerSession } from 'next-auth'
import { authOptions } from '@/lib/auth'

// Interfaces
interface AvatarRenderRequest {
  projectId: string
  slideId: string
  avatarConfig: {
    model: string
    pose: string
    expression: number
    gestureFrequency: number
    eyeContact: number
    quality: 'draft' | 'standard' | 'high' | 'ultra'
  }
  audioUrl: string
  audioText: string
  duration: number
  lipSyncEnabled: boolean
  gestureSync: boolean
}

interface AvatarRenderJob {
  id: string
  projectId: string
  slideId: string
  status: 'queued' | 'analyzing' | 'generating' | 'lip-sync' | 'rendering' | 'completed' | 'failed'
  progress: number
  currentStage: string
  outputUrl?: string
  thumbnailUrl?: string
  errorMessage?: string
  audioAnalysis?: AudioAnalysis
  lipSyncData?: LipSyncData
  gestureData?: GestureData
  createdAt: Date
  completedAt?: Date
}

interface AudioAnalysis {
  duration: number
  sampleRate: number
  phonemes: Array<{
    phoneme: string
    startTime: number
    endTime: number
    intensity: number
  }>
  emotions: Array<{
    emotion: string
    confidence: number
    startTime: number
    endTime: number
  }>
  speechRate: number
  pausePoints: number[]
}

interface LipSyncData {
  keyframes: Array<{
    time: number
    viseme: string
    intensity: number
    mouthShape: {
      openness: number
      width: number
      lipPosition: number
    }
  }>
  blendShapes: Array<{
    name: string
    values: Array<{
      time: number
      value: number
    }>
  }>
}

interface GestureData {
  gestures: Array<{
    type: 'hand' | 'head' | 'body'
    startTime: number
    endTime: number
    intensity: number
    keyframes: Array<{
      time: number
      position: { x: number; y: number; z: number }
      rotation: { x: number; y: number; z: number }
    }>
  }>
}

// Avatar 3D Renderer
class Avatar3DRenderer {
  private renderJobs: Map<string, AvatarRenderJob> = new Map()

  // Create avatar render job
  async createAvatarRenderJob(request: AvatarRenderRequest, userId: string): Promise<AvatarRenderJob> {
    const jobId = `avatar_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    
    const renderJob: AvatarRenderJob = {
      id: jobId,
      projectId: request.projectId,
      slideId: request.slideId,
      status: 'queued',
      progress: 0,
      currentStage: 'Preparando renderiza√ß√£o do avatar...',
      createdAt: new Date()
    }

    this.renderJobs.set(jobId, renderJob)

    // Start processing asynchronously
    this.processAvatarRender(jobId, request).catch(error => {
      console.error('Avatar render failed:', error)
      this.updateRenderJob(jobId, {
        status: 'failed',
        errorMessage: error.message
      })
    })

    return renderJob
  }

  // Process avatar render
  private async processAvatarRender(jobId: string, request: AvatarRenderRequest): Promise<void> {
    try {
      // Stage 1: Analyze audio
      await this.updateRenderJob(jobId, {
        status: 'analyzing',
        progress: 10,
        currentStage: 'Analisando √°udio para sincroniza√ß√£o...'
      })

      const audioAnalysis = await this.analyzeAudio(request.audioUrl, request.audioText)
      await this.updateRenderJob(jobId, { audioAnalysis })

      // Stage 2: Generate lip-sync data
      if (request.lipSyncEnabled) {
        await this.updateRenderJob(jobId, {
          status: 'lip-sync',
          progress: 30,
          currentStage: 'Gerando dados de sincroniza√ß√£o labial...'
        })

        const lipSyncData = await this.generateLipSync(audioAnalysis, request.audioText)
        await this.updateRenderJob(jobId, { lipSyncData })
      }

      // Stage 3: Generate gesture data
      if (request.gestureSync) {
        await this.updateRenderJob(jobId, {
          progress: 50,
          currentStage: 'Gerando gestos e movimentos...'
        })

        const gestureData = await this.generateGestures(audioAnalysis, request.avatarConfig)
        await this.updateRenderJob(jobId, { gestureData })
      }

      // Stage 4: Render avatar
      await this.updateRenderJob(jobId, {
        status: 'rendering',
        progress: 70,
        currentStage: 'Renderizando avatar 3D...'
      })

      const renderResult = await this.renderAvatar(request, audioAnalysis, 
        this.renderJobs.get(jobId)?.lipSyncData, 
        this.renderJobs.get(jobId)?.gestureData)

      // Stage 5: Complete
      await this.updateRenderJob(jobId, {
        status: 'completed',
        progress: 100,
        currentStage: 'Avatar renderizado com sucesso!',
        outputUrl: renderResult.videoUrl,
        thumbnailUrl: renderResult.thumbnailUrl,
        completedAt: new Date()
      })

    } catch (error: any) {
      await this.updateRenderJob(jobId, {
        status: 'failed',
        errorMessage: error.message,
        currentStage: 'Erro na renderiza√ß√£o do avatar'
      })
      throw error
    }
  }

  // Analyze audio for lip-sync and gestures
  private async analyzeAudio(audioUrl: string, audioText: string): Promise<AudioAnalysis> {
    // Simulate audio analysis
    await new Promise(resolve => setTimeout(resolve, 2000))

    // Generate phonemes from text
    const phonemes = this.textToPhonemes(audioText)
    
    // Simulate emotion detection
    const emotions = this.detectEmotions(audioText)

    // Calculate speech rate and pauses
    const speechRate = audioText.length / 60 // chars per second (simulated)
    const pausePoints = this.detectPauses(audioText)

    return {
      duration: 60, // Simulated duration
      sampleRate: 44100,
      phonemes,
      emotions,
      speechRate,
      pausePoints
    }
  }

  // Convert text to phonemes
  private textToPhonemes(text: string): AudioAnalysis['phonemes'] {
    const words = text.split(' ')
    const phonemes: AudioAnalysis['phonemes'] = []
    let currentTime = 0

    for (const word of words) {
      // Simulate phoneme generation for each word
      const wordPhonemes = this.wordToPhonemes(word)
      for (const phoneme of wordPhonemes) {
        phonemes.push({
          phoneme: phoneme.phoneme,
          startTime: currentTime + phoneme.offset,
          endTime: currentTime + phoneme.offset + phoneme.duration,
          intensity: phoneme.intensity
        })
      }
      currentTime += word.length * 0.1 + 0.2 // Simulate timing
    }

    return phonemes
  }

  private wordToPhonemes(word: string): Array<{phoneme: string; offset: number; duration: number; intensity: number}> {
    // Simplified phoneme mapping
    const phonemeMap: {[key: string]: string[]} = {
      'a': ['AH'], 'e': ['EH'], 'i': ['IH'], 'o': ['OH'], 'u': ['UH'],
      'b': ['B'], 'c': ['K'], 'd': ['D'], 'f': ['F'], 'g': ['G'],
      'h': ['HH'], 'j': ['JH'], 'k': ['K'], 'l': ['L'], 'm': ['M'],
      'n': ['N'], 'p': ['P'], 'q': ['K'], 'r': ['R'], 's': ['S'],
      't': ['T'], 'v': ['V'], 'w': ['W'], 'x': ['KS'], 'y': ['Y'], 'z': ['Z']
    }

    const phonemes = []
    let offset = 0

    for (const char of word.toLowerCase()) {
      const phoneme = phonemeMap[char]?.[0] || 'SIL'
      phonemes.push({
        phoneme,
        offset,
        duration: 0.1,
        intensity: Math.random() * 0.5 + 0.5
      })
      offset += 0.1
    }

    return phonemes
  }

  // Detect emotions in text
  private detectEmotions(text: string): AudioAnalysis['emotions'] {
    // Simplified emotion detection
    const emotions = []
    const words = text.split(' ')
    let currentTime = 0

    // Detect emotional keywords
    const emotionKeywords = {
      'happy': ['feliz', 'alegre', '√≥timo', 'excelente', 'maravilhoso'],
      'sad': ['triste', 'ruim', 'p√©ssimo', 'terr√≠vel'],
      'excited': ['incr√≠vel', 'fant√°stico', 'empolgante', 'animado'],
      'calm': ['calmo', 'tranquilo', 'sereno', 'pac√≠fico']
    }

    for (const word of words) {
      for (const [emotion, keywords] of Object.entries(emotionKeywords)) {
        if (keywords.some(keyword => word.toLowerCase().includes(keyword))) {
          emotions.push({
            emotion,
            confidence: 0.8,
            startTime: currentTime,
            endTime: currentTime + word.length * 0.1
          })
        }
      }
      currentTime += word.length * 0.1 + 0.2
    }

    // Default neutral emotion if no emotions detected
    if (emotions.length === 0) {
      emotions.push({
        emotion: 'neutral',
        confidence: 0.9,
        startTime: 0,
        endTime: currentTime
      })
    }

    return emotions
  }

  // Detect pause points in text
  private detectPauses(text: string): number[] {
    const pausePoints = []
    let currentTime = 0

    for (const char of text) {
      if (char === '.' || char === ',' || char === ';' || char === '!') {
        pausePoints.push(currentTime)
      }
      currentTime += 0.1
    }

    return pausePoints
  }

  // Generate lip-sync data
  private async generateLipSync(audioAnalysis: AudioAnalysis, text: string): Promise<LipSyncData> {
    // Simulate lip-sync generation
    await new Promise(resolve => setTimeout(resolve, 1500))

    const keyframes: LipSyncData['keyframes'] = []
    const blendShapes: LipSyncData['blendShapes'] = []

    // Generate keyframes from phonemes
    for (const phoneme of audioAnalysis.phonemes) {
      const mouthShape = this.phonemeToMouthShape(phoneme.phoneme)
      
      keyframes.push({
        time: phoneme.startTime,
        viseme: phoneme.phoneme,
        intensity: phoneme.intensity,
        mouthShape
      })
    }

    // Generate blend shapes for smooth animation
    const blendShapeNames = ['jawOpen', 'mouthClose', 'mouthFunnel', 'mouthPucker', 'mouthSmile']
    
    for (const shapeName of blendShapeNames) {
      const values = keyframes.map(kf => ({
        time: kf.time,
        value: this.calculateBlendShapeValue(shapeName, kf.viseme, kf.intensity)
      }))
      
      blendShapes.push({ name: shapeName, values })
    }

    return { keyframes, blendShapes }
  }

  // Convert phoneme to mouth shape
  private phonemeToMouthShape(phoneme: string): LipSyncData['keyframes'][0]['mouthShape'] {
    const mouthShapes: {[key: string]: {openness: number; width: number; lipPosition: number}} = {
      'AH': { openness: 0.8, width: 0.6, lipPosition: 0.0 },
      'EH': { openness: 0.4, width: 0.8, lipPosition: 0.2 },
      'IH': { openness: 0.2, width: 0.9, lipPosition: 0.4 },
      'OH': { openness: 0.6, width: 0.3, lipPosition: -0.2 },
      'UH': { openness: 0.3, width: 0.2, lipPosition: -0.4 },
      'B': { openness: 0.0, width: 0.5, lipPosition: 0.0 },
      'M': { openness: 0.0, width: 0.5, lipPosition: 0.0 },
      'P': { openness: 0.0, width: 0.5, lipPosition: 0.0 },
      'F': { openness: 0.1, width: 0.7, lipPosition: 0.3 },
      'V': { openness: 0.1, width: 0.7, lipPosition: 0.3 },
      'SIL': { openness: 0.0, width: 0.5, lipPosition: 0.0 }
    }

    return mouthShapes[phoneme] || mouthShapes['SIL']
  }

  // Calculate blend shape value
  private calculateBlendShapeValue(shapeName: string, viseme: string, intensity: number): number {
    const shapeInfluence: {[key: string]: {[key: string]: number}} = {
      'jawOpen': { 'AH': 0.8, 'OH': 0.6, 'EH': 0.4 },
      'mouthClose': { 'B': 1.0, 'M': 1.0, 'P': 1.0 },
      'mouthFunnel': { 'OH': 0.7, 'UH': 0.8 },
      'mouthPucker': { 'UH': 0.9, 'OH': 0.5 },
      'mouthSmile': { 'IH': 0.6, 'EH': 0.4 }
    }

    const influence = shapeInfluence[shapeName]?.[viseme] || 0
    return influence * intensity
  }

  // Generate gesture data
  private async generateGestures(audioAnalysis: AudioAnalysis, avatarConfig: AvatarRenderRequest['avatarConfig']): Promise<GestureData> {
    // Simulate gesture generation
    await new Promise(resolve => setTimeout(resolve, 1000))

    const gestures: GestureData['gestures'] = []

    // Generate gestures based on emotions and speech rate
    for (const emotion of audioAnalysis.emotions) {
      const gestureType = this.emotionToGestureType(emotion.emotion)
      const intensity = emotion.confidence * (avatarConfig.gestureFrequency / 100)

      gestures.push({
        type: gestureType,
        startTime: emotion.startTime,
        endTime: emotion.endTime,
        intensity,
        keyframes: this.generateGestureKeyframes(gestureType, emotion.startTime, emotion.endTime, intensity)
      })
    }

    // Add periodic head movements
    const headGestures = this.generateHeadMovements(audioAnalysis.duration, avatarConfig.eyeContact / 100)
    gestures.push(...headGestures)

    return { gestures }
  }

  private emotionToGestureType(emotion: string): 'hand' | 'head' | 'body' {
    const gestureMap: {[key: string]: 'hand' | 'head' | 'body'} = {
      'happy': 'hand',
      'excited': 'hand',
      'sad': 'head',
      'calm': 'body',
      'neutral': 'head'
    }
    return gestureMap[emotion] || 'head'
  }

  private generateGestureKeyframes(type: 'hand' | 'head' | 'body', startTime: number, endTime: number, intensity: number): GestureData['gestures'][0]['keyframes'] {
    const keyframes = []
    const duration = endTime - startTime
    const steps = Math.max(2, Math.floor(duration * 2)) // 2 keyframes per second

    for (let i = 0; i <= steps; i++) {
      const time = startTime + (duration * i / steps)
      const progress = i / steps

      let position, rotation

      switch (type) {
        case 'hand':
          position = {
            x: Math.sin(progress * Math.PI * 2) * intensity * 0.2,
            y: Math.cos(progress * Math.PI * 2) * intensity * 0.1,
            z: 0
          }
          rotation = {
            x: Math.sin(progress * Math.PI) * intensity * 15,
            y: 0,
            z: Math.cos(progress * Math.PI) * intensity * 10
          }
          break

        case 'head':
          position = { x: 0, y: 0, z: 0 }
          rotation = {
            x: Math.sin(progress * Math.PI * 0.5) * intensity * 5,
            y: Math.cos(progress * Math.PI * 0.3) * intensity * 8,
            z: Math.sin(progress * Math.PI * 0.7) * intensity * 3
          }
          break

        case 'body':
          position = {
            x: 0,
            y: Math.sin(progress * Math.PI) * intensity * 0.05,
            z: 0
          }
          rotation = {
            x: Math.sin(progress * Math.PI * 0.2) * intensity * 2,
            y: 0,
            z: Math.cos(progress * Math.PI * 0.3) * intensity * 1
          }
          break
      }

      keyframes.push({ time, position, rotation })
    }

    return keyframes
  }

  private generateHeadMovements(duration: number, eyeContactLevel: number): GestureData['gestures'] {
    const movements = []
    const movementInterval = 3 // Every 3 seconds
    const numMovements = Math.floor(duration / movementInterval)

    for (let i = 0; i < numMovements; i++) {
      const startTime = i * movementInterval
      const endTime = startTime + 1 // 1 second movement

      movements.push({
        type: 'head' as const,
        startTime,
        endTime,
        intensity: eyeContactLevel,
        keyframes: [
          {
            time: startTime,
            position: { x: 0, y: 0, z: 0 },
            rotation: { x: 0, y: 0, z: 0 }
          },
          {
            time: startTime + 0.5,
            position: { x: 0, y: 0, z: 0 },
            rotation: { 
              x: (Math.random() - 0.5) * 10 * eyeContactLevel,
              y: (Math.random() - 0.5) * 15 * eyeContactLevel,
              z: (Math.random() - 0.5) * 5 * eyeContactLevel
            }
          },
          {
            time: endTime,
            position: { x: 0, y: 0, z: 0 },
            rotation: { x: 0, y: 0, z: 0 }
          }
        ]
      })
    }

    return movements
  }

  // Render avatar with all data
  private async renderAvatar(
    request: AvatarRenderRequest, 
    audioAnalysis: AudioAnalysis,
    lipSyncData?: LipSyncData,
    gestureData?: GestureData
  ): Promise<{videoUrl: string; thumbnailUrl: string}> {
    // Simulate avatar rendering
    await new Promise(resolve => setTimeout(resolve, 3000))

    const outputFilename = `avatar_${request.slideId}_${Date.now()}.mp4`
    const thumbnailFilename = `avatar_${request.slideId}_${Date.now()}_thumb.jpg`

    return {
      videoUrl: `/api/files/avatars/${outputFilename}`,
      thumbnailUrl: `/api/files/avatars/${thumbnailFilename}`
    }
  }

  // Update render job
  private async updateRenderJob(jobId: string, updates: Partial<AvatarRenderJob>): Promise<void> {
    const job = this.renderJobs.get(jobId)
    if (job) {
      Object.assign(job, updates)
      this.renderJobs.set(jobId, job)
    }
  }

  // Get render job
  getRenderJob(jobId: string): AvatarRenderJob | undefined {
    return this.renderJobs.get(jobId)
  }
}

// Global instance
const avatarRenderer = new Avatar3DRenderer()

// API Handlers
export async function POST(request: NextRequest) {
  try {
    const session = await getServerSession(authOptions)
    if (!session?.user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    const body: AvatarRenderRequest = await request.json()

    // Validate request
    if (!body.projectId || !body.slideId || !body.audioUrl) {
      return NextResponse.json({ 
        error: 'Project ID, slide ID, and audio URL are required' 
      }, { status: 400 })
    }

    // Create avatar render job
    const renderJob = await avatarRenderer.createAvatarRenderJob(body, session.user.id)

    return NextResponse.json({
      success: true,
      renderJob: {
        id: renderJob.id,
        status: renderJob.status,
        progress: renderJob.progress,
        currentStage: renderJob.currentStage
      }
    })

  } catch (error: any) {
    console.error('Avatar render API error:', error)
    return NextResponse.json({ 
      error: 'Internal server error',
      details: error.message 
    }, { status: 500 })
  }
}

export async function GET(request: NextRequest) {
  try {
    const session = await getServerSession(authOptions)
    if (!session?.user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    const { searchParams } = new URL(request.url)
    const jobId = searchParams.get('jobId')

    if (!jobId) {
      return NextResponse.json({ 
        error: 'Job ID is required' 
      }, { status: 400 })
    }

    const renderJob = avatarRenderer.getRenderJob(jobId)

    if (!renderJob) {
      return NextResponse.json({ 
        error: 'Avatar render job not found' 
      }, { status: 404 })
    }

    return NextResponse.json({
      success: true,
      renderJob: {
        id: renderJob.id,
        projectId: renderJob.projectId,
        slideId: renderJob.slideId,
        status: renderJob.status,
        progress: renderJob.progress,
        currentStage: renderJob.currentStage,
        outputUrl: renderJob.outputUrl,
        thumbnailUrl: renderJob.thumbnailUrl,
        errorMessage: renderJob.errorMessage,
        createdAt: renderJob.createdAt,
        completedAt: renderJob.completedAt
      }
    })

  } catch (error: any) {
    console.error('Avatar render status API error:', error)
    return NextResponse.json({ 
      error: 'Internal server error',
      details: error.message 
    }, { status: 500 })
  }
}