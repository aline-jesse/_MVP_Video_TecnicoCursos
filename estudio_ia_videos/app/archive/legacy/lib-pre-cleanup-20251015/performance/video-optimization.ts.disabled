

/**
 * Video Performance Optimization System
 * Handles caching, warm-up, and batch processing for Sprint 5
 */

export interface OptimizationConfig {
  enable_model_warmup: boolean
  enable_batch_processing: boolean
  enable_preview_cache: boolean
  enable_tts_cache: boolean
  max_concurrent_jobs: number
  cost_threshold_per_job: number
  retention_days: number
}

export interface CacheEntry {
  key: string
  data: any
  created_at: number
  ttl: number
  size: number
  hit_count: number
}

/**
 * Advanced Caching System
 */
export class VideoOptimizationCache {
  private cache: Map<string, CacheEntry> = new Map()
  private totalSize = 0
  private maxSize = 500 * 1024 * 1024 // 500MB max cache
  private hitCount = 0
  private missCount = 0

  /**
   * Cache TTS audio by text hash
   */
  async cacheTTS(text: string, voice: string, audioBuffer: ArrayBuffer): Promise<void> {
    const key = this.generateTTSKey(text, voice)
    const entry: CacheEntry = {
      key,
      data: audioBuffer,
      created_at: Date.now(),
      ttl: 24 * 60 * 60 * 1000, // 24 hours
      size: audioBuffer.byteLength,
      hit_count: 0
    }

    // Check if we need to clean up cache
    if (this.totalSize + entry.size > this.maxSize) {
      await this.cleanupCache()
    }

    this.cache.set(key, entry)
    this.totalSize += entry.size

    console.log(`TTS cached: ${key} (${(entry.size / 1024).toFixed(1)}KB)`)
  }

  /**
   * Get cached TTS audio
   */
  async getCachedTTS(text: string, voice: string): Promise<ArrayBuffer | null> {
    const key = this.generateTTSKey(text, voice)
    const entry = this.cache.get(key)

    if (!entry) {
      this.missCount++
      return null
    }

    // Check if expired
    if (Date.now() - entry.created_at > entry.ttl) {
      this.cache.delete(key)
      this.totalSize -= entry.size
      this.missCount++
      return null
    }

    entry.hit_count++
    this.hitCount++
    
    console.log(`TTS cache hit: ${key}`)
    return entry.data as ArrayBuffer
  }

  /**
   * Cache video generation result
   */
  async cacheVideoGeneration(params: any, result: any): Promise<void> {
    const key = this.generateVideoKey(params)
    const entry: CacheEntry = {
      key,
      data: result,
      created_at: Date.now(),
      ttl: 2 * 60 * 60 * 1000, // 2 hours for videos
      size: JSON.stringify(result).length,
      hit_count: 0
    }

    this.cache.set(key, entry)
    this.totalSize += entry.size

    console.log(`Video generation cached: ${key}`)
  }

  /**
   * Get cached video generation
   */
  async getCachedVideoGeneration(params: any): Promise<any | null> {
    const key = this.generateVideoKey(params)
    const entry = this.cache.get(key)

    if (!entry || Date.now() - entry.created_at > entry.ttl) {
      this.missCount++
      return null
    }

    entry.hit_count++
    this.hitCount++
    return entry.data
  }

  /**
   * Generate cache key for TTS
   */
  private generateTTSKey(text: string, voice: string): string {
    // Create hash from text and voice
    const content = `${text}-${voice}`
    let hash = 0
    for (let i = 0; i < content.length; i++) {
      const char = content.charCodeAt(i)
      hash = ((hash << 5) - hash) + char
      hash = hash & hash // Convert to 32bit integer
    }
    return `tts-${Math.abs(hash)}`
  }

  /**
   * Generate cache key for video generation
   */
  private generateVideoKey(params: any): string {
    const keyData = {
      prompt: params.prompt,
      duration: params.duration,
      style: params.style,
      resolution: params.resolution,
      seed: params.seed
    }
    
    const content = JSON.stringify(keyData)
    let hash = 0
    for (let i = 0; i < content.length; i++) {
      const char = content.charCodeAt(i)
      hash = ((hash << 5) - hash) + char
      hash = hash & hash
    }
    return `video-${Math.abs(hash)}`
  }

  /**
   * Cleanup expired and least used entries
   */
  private async cleanupCache(): Promise<void> {
    const now = Date.now()
    const entriesToRemove: string[] = []
    
    // Remove expired entries
    for (const [key, entry] of this.cache.entries()) {
      if (now - entry.created_at > entry.ttl) {
        entriesToRemove.push(key)
        this.totalSize -= entry.size
      }
    }

    // Remove least used entries if still over limit
    if (this.totalSize > this.maxSize * 0.8) {
      const entries = Array.from(this.cache.entries())
        .sort((a, b) => a[1].hit_count - b[1].hit_count)
      
      const toRemove = entries.slice(0, Math.ceil(entries.length * 0.3))
      toRemove.forEach(([key, entry]) => {
        entriesToRemove.push(key)
        this.totalSize -= entry.size
      })
    }

    // Remove entries
    entriesToRemove.forEach(key => this.cache.delete(key))

    console.log(`Cache cleanup: removed ${entriesToRemove.length} entries`)
  }

  /**
   * Get cache statistics
   */
  getCacheStats(): {
    total_entries: number
    total_size_mb: number
    hit_rate: number
    oldest_entry_hours: number
    newest_entry_hours: number
  } {
    const entries = Array.from(this.cache.values())
    const now = Date.now()
    
    const oldest = entries.length > 0 
      ? Math.min(...entries.map(e => e.created_at))
      : now
    
    const newest = entries.length > 0
      ? Math.max(...entries.map(e => e.created_at))
      : now

    return {
      total_entries: this.cache.size,
      total_size_mb: this.totalSize / (1024 * 1024),
      hit_rate: this.hitCount + this.missCount > 0 
        ? this.hitCount / (this.hitCount + this.missCount)
        : 0,
      oldest_entry_hours: (now - oldest) / (1000 * 60 * 60),
      newest_entry_hours: (now - newest) / (1000 * 60 * 60)
    }
  }

  /**
   * Clear all cache
   */
  clearCache(): void {
    this.cache.clear()
    this.totalSize = 0
    this.hitCount = 0
    this.missCount = 0
    console.log('Cache cleared')
  }
}

/**
 * Model Warm-up System
 */
export class ModelWarmupManager {
  private warmupStatus: Map<string, boolean> = new Map()
  private warmupPromises: Map<string, Promise<void>> = new Map()

  /**
   * Warm up video generation models
   */
  async warmupVideoModels(): Promise<void> {
    const models = ['ltx-video', 'hunyuan-video']
    
    console.log('Starting model warm-up...')
    
    const warmupPromises = models.map(model => this.warmupModel(model))
    await Promise.allSettled(warmupPromises)
    
    console.log('Model warm-up completed')
  }

  /**
   * Warm up individual model
   */
  private async warmupModel(modelName: string): Promise<void> {
    if (this.warmupStatus.get(modelName)) {
      return // Already warmed up
    }

    const warmupPromise = this.performWarmup(modelName)
    this.warmupPromises.set(modelName, warmupPromise)
    
    try {
      await warmupPromise
      this.warmupStatus.set(modelName, true)
      console.log(`Model ${modelName} warmed up successfully`)
    } catch (error) {
      console.error(`Warm-up failed for ${modelName}:`, error)
    } finally {
      this.warmupPromises.delete(modelName)
    }
  }

  /**
   * Perform actual model warm-up
   */
  private async performWarmup(modelName: string): Promise<void> {
    // Generate a small test video to initialize the model
    const warmupParams = {
      prompt: 'Test warm-up video for model initialization',
      duration: 2,
      resolution: '720p',
      style: 'realistic'
    }

    try {
      // This would call the actual model API
      console.log(`Warming up ${modelName} with test generation...`)
      
      // Simulate warmup time
      await new Promise(resolve => setTimeout(resolve, 3000))
      
    } catch (error) {
      // Warmup failures are logged but don't block normal operation
      console.warn(`Warmup for ${modelName} failed, will initialize on first use`)
    }
  }

  /**
   * Check if model is ready
   */
  isModelReady(modelName: string): boolean {
    return this.warmupStatus.get(modelName) || false
  }

  /**
   * Wait for model to be ready
   */
  async waitForModel(modelName: string): Promise<void> {
    const warmupPromise = this.warmupPromises.get(modelName)
    if (warmupPromise) {
      await warmupPromise
    }
  }
}

/**
 * Batch Processing Manager
 */
export class BatchProcessingManager {
  private batchQueue: Array<{
    id: string
    type: 'video_generation' | 'avatar_render' | 'tts_generation'
    params: any
    callback: (result: any) => void
  }> = []
  
  private batchSize = 3
  private processingInterval = 5000 // 5 seconds

  constructor() {
    this.startBatchProcessor()
  }

  /**
   * Add job to batch queue
   */
  addToBatch(
    type: 'video_generation' | 'avatar_render' | 'tts_generation',
    params: any
  ): Promise<any> {
    return new Promise((resolve, reject) => {
      const job = {
        id: `batch-${Date.now()}-${Math.random().toString(36).substr(2, 5)}`,
        type,
        params,
        callback: resolve
      }

      this.batchQueue.push(job)
      console.log(`Added job ${job.id} to batch queue (${this.batchQueue.length} total)`)
    })
  }

  /**
   * Start batch processor
   */
  private startBatchProcessor(): void {
    setInterval(() => {
      if (this.batchQueue.length >= this.batchSize) {
        this.processBatch()
      }
    }, this.processingInterval)

    // Also process smaller batches if they've been waiting too long
    setInterval(() => {
      if (this.batchQueue.length > 0) {
        const oldestJob = this.batchQueue[0]
        const waitTime = Date.now() - parseInt(oldestJob.id.split('-')[1])
        
        if (waitTime > 30000) { // 30 seconds
          this.processBatch()
        }
      }
    }, 10000)
  }

  /**
   * Process batch of jobs
   */
  private async processBatch(): Promise<void> {
    const batch = this.batchQueue.splice(0, this.batchSize)
    
    if (batch.length === 0) return

    console.log(`Processing batch of ${batch.length} jobs`)

    // Group by type for efficient processing
    const groupedJobs = batch.reduce((groups, job) => {
      if (!groups[job.type]) groups[job.type] = []
      groups[job.type].push(job)
      return groups
    }, {} as Record<string, typeof batch>)

    // Process each group
    for (const [type, jobs] of Object.entries(groupedJobs)) {
      try {
        await this.processJobGroup(type, jobs)
      } catch (error) {
        console.error(`Batch processing error for ${type}:`, error)
        
        // Fail all jobs in this group
        jobs.forEach(job => {
          job.callback(new Error(`Batch processing failed: ${error}`))
        })
      }
    }
  }

  /**
   * Process group of similar jobs
   */
  private async processJobGroup(type: string, jobs: any[]): Promise<void> {
    switch (type) {
      case 'video_generation':
        await this.processBatchVideoGeneration(jobs)
        break
      case 'avatar_render':
        await this.processBatchAvatarRender(jobs)
        break
      case 'tts_generation':
        await this.processBatchTTSGeneration(jobs)
        break
    }
  }

  private async processBatchVideoGeneration(jobs: any[]): Promise<void> {
    // Process video generation jobs in parallel
    const promises = jobs.map(async (job) => {
      try {
        // Simulate video generation
        const result = {
          video_url: `/batch-videos/${job.id}.mp4`,
          duration: job.params.duration,
          cost: job.params.duration * 0.05
        }
        
        job.callback(result)
      } catch (error) {
        job.callback(error)
      }
    })

    await Promise.allSettled(promises)
  }

  private async processBatchAvatarRender(jobs: any[]): Promise<void> {
    // Process avatar renders - can share 3D context
    for (const job of jobs) {
      try {
        const result = {
          video_url: `/batch-avatars/${job.id}.mp4`,
          sync_quality: 0.94,
          cost: job.params.duration * 0.03
        }
        
        job.callback(result)
      } catch (error) {
        job.callback(error)
      }
    }
  }

  private async processBatchTTSGeneration(jobs: any[]): Promise<void> {
    // Batch TTS can be more efficient
    const promises = jobs.map(async (job) => {
      try {
        const result = {
          audio_url: `/batch-tts/${job.id}.wav`,
          duration: job.params.text.length * 0.08,
          cost: 0.002 // Fixed cost for TTS
        }
        
        job.callback(result)
      } catch (error) {
        job.callback(error)
      }
    })

    await Promise.allSettled(promises)
  }

  /**
   * Generate TTS cache key
   */
  private generateTTSKey(text: string, voice: string): string {
    return `tts-${this.hashString(text + voice)}`
  }

  /**
   * Simple string hashing
   */
  private hashString(str: string): string {
    let hash = 0
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i)
      hash = ((hash << 5) - hash) + char
      hash = hash & hash
    }
    return Math.abs(hash).toString(36)
  }

  /**
   * Cleanup old cache entries
   */
  private async cleanupCache(): Promise<void> {
    const now = Date.now()
    const entriesToRemove: string[] = []
    
    // Remove expired entries first
    for (const [key, entry] of this.cache.entries()) {
      if (now - entry.created_at > entry.ttl) {
        entriesToRemove.push(key)
        this.totalSize -= entry.size
      }
    }

    // If still over limit, remove least used entries
    if (this.totalSize > this.maxSize * 0.8) {
      const entries = Array.from(this.cache.entries())
        .filter(([key]) => !entriesToRemove.includes(key))
        .sort((a, b) => a[1].hit_count - b[1].hit_count)
      
      const additionalRemoval = entries.slice(0, Math.ceil(entries.length * 0.2))
      additionalRemoval.forEach(([key, entry]) => {
        entriesToRemove.push(key)
        this.totalSize -= entry.size
      })
    }

    // Remove entries
    entriesToRemove.forEach(key => this.cache.delete(key))

    console.log(`Cache cleanup: removed ${entriesToRemove.length} entries, ${(this.totalSize / 1024 / 1024).toFixed(1)}MB remaining`)
  }

  /**
   * Get cache performance metrics
   */
  getPerformanceMetrics(): {
    cache_size_mb: number
    total_entries: number
    hit_rate: number
    memory_usage: number
    oldest_entry_hours: number
  } {
    const entries = Array.from(this.cache.values())
    const now = Date.now()
    
    const oldest = entries.length > 0 
      ? Math.min(...entries.map(e => e.created_at))
      : now

    return {
      cache_size_mb: this.totalSize / (1024 * 1024),
      total_entries: this.cache.size,
      hit_rate: this.hitCount + this.missCount > 0 
        ? this.hitCount / (this.hitCount + this.missCount)
        : 0,
      memory_usage: this.totalSize / this.maxSize,
      oldest_entry_hours: (now - oldest) / (1000 * 60 * 60)
    }
  }
}

/**
 * Performance Monitor
 */
export class PerformanceMonitor {
  private metrics: Map<string, any[]> = new Map()
  private maxMetrics = 1000 // Keep last 1000 measurements

  /**
   * Record performance metric
   */
  recordMetric(name: string, value: number, metadata?: any): void {
    if (!this.metrics.has(name)) {
      this.metrics.set(name, [])
    }

    const metrics = this.metrics.get(name)!
    metrics.push({
      value,
      timestamp: Date.now(),
      metadata: metadata || {}
    })

    // Keep only recent metrics
    if (metrics.length > this.maxMetrics) {
      metrics.splice(0, metrics.length - this.maxMetrics)
    }
  }

  /**
   * Get metric statistics
   */
  getMetricStats(name: string, timeWindowMs = 60 * 60 * 1000): {
    avg: number
    min: number
    max: number
    count: number
    p95: number
    p99: number
  } {
    const metrics = this.metrics.get(name) || []
    const cutoff = Date.now() - timeWindowMs
    const recentMetrics = metrics
      .filter(m => m.timestamp > cutoff)
      .map(m => m.value)
      .sort((a, b) => a - b)

    if (recentMetrics.length === 0) {
      return { avg: 0, min: 0, max: 0, count: 0, p95: 0, p99: 0 }
    }

    const sum = recentMetrics.reduce((a, b) => a + b, 0)
    const p95Index = Math.floor(recentMetrics.length * 0.95)
    const p99Index = Math.floor(recentMetrics.length * 0.99)

    return {
      avg: sum / recentMetrics.length,
      min: recentMetrics[0],
      max: recentMetrics[recentMetrics.length - 1],
      count: recentMetrics.length,
      p95: recentMetrics[p95Index] || recentMetrics[recentMetrics.length - 1],
      p99: recentMetrics[p99Index] || recentMetrics[recentMetrics.length - 1]
    }
  }

  /**
   * Get all available metrics
   */
  getAllMetrics(): string[] {
    return Array.from(this.metrics.keys())
  }
}

// Global instances
export const optimizationCache = new VideoOptimizationCache()
export const modelWarmupManager = new ModelWarmupManager()
export const performanceMonitor = new PerformanceMonitor()

// Initialize warm-up on module load
modelWarmupManager.warmupVideoModels().catch(console.error)
