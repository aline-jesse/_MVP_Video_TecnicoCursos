
/**
 * üé≠ Pipeline de Avatares 3D Hiper-Realistas
 * Engine de renderiza√ß√£o cinematogr√°fica com Audio2Face Integration
 * FASE 2: Sprint 1 - Audio2Face Integration
 */

import { 
  audio2FaceService, 
  type BlendShapesData, 
  type Audio2FaceConfig,
  type Audio2FaceSession,
  defaultAudio2FaceConfig 
} from './services/audio2face-service'
import { createClient } from '@supabase/supabase-js'
import type { Database } from '@/lib/supabase/types'

interface Avatar3D {
  id: string
  name: string
  category: 'business' | 'healthcare' | 'education' | 'casual' | 'safety'
  gender: 'male' | 'female' | 'unisex'
  ethnicity: 'caucasian' | 'afro' | 'asian' | 'latino' | 'mixed'
  age: 'young' | 'adult' | 'senior'
  style: 'realistic' | 'stylized' | 'cartoon'
  quality: 'standard' | 'premium' | 'cinematic' | 'hyperreal'
  
  // Recursos visuais hiper-realistas
  features: {
    facialDetails: 'high' | 'ultra' | 'cinematic'
    skinTexture: 'procedural' | 'scanned' | 'photogrammetry'
    hairSystem: 'strand' | 'cards' | 'volumetric'
    clothingPhysics: boolean
    microExpressions: boolean
    eyeTracking: boolean
    breathingAnimation: boolean
    lipSyncAccuracy: number // 0-100%
  }
  
  // Assets 3D
  assets: {
    modelFile: string // .fbx, .gltf
    textureFiles: string[] // 4K, 8K textures
    rigFile: string // Skeleton rig
    animationSet: string[] // Facial animations
    blendShapes: string[] // Facial expressions
    materialFiles: string[] // PBR materials
  }
  
  // Configura√ß√µes de renderiza√ß√£o
  rendering: {
    resolution: '2K' | '4K' | '8K'
    antiAliasing: 'FXAA' | 'MSAA' | 'TAA'
    rayTracing: boolean
    globalIllumination: boolean
    subsurfaceScattering: boolean
    volumetricLighting: boolean
    motionBlur: boolean
    depthOfField: boolean
  }
  
  // Anima√ß√µes avan√ßadas
  animations: {
    idle: string[]
    talking: string[]
    gestures: string[]
    emotions: string[]
    transitions: string[]
  }
  
  // Configura√ß√µes de voice sync
  voiceSync: {
    supportedLanguages: string[]
    phonemeMapping: Record<string, string>
    lipSyncEngine: 'basic' | 'advanced' | 'ml-driven'
    facialRig: string
  }
}

interface Avatar3DScene {
  avatar: Avatar3D
  position: { x: number; y: number; z: number }
  rotation: { x: number; y: number; z: number }
  scale: { x: number; y: number; z: number }
  animation: string
  emotion: string
  lookAt: { x: number; y: number; z: number }
  
  // Ilumina√ß√£o cinematogr√°fica
  lighting: {
    keyLight: { intensity: number; color: string; angle: number }
    fillLight: { intensity: number; color: string; angle: number }
    rimLight: { intensity: number; color: string; angle: number }
    ambient: { intensity: number; color: string }
  }
  
  // Camera cinematogr√°fica
  camera: {
    position: { x: number; y: number; z: number }
    target: { x: number; y: number; z: number }
    fov: number
    aperture: number
    focusDistance: number
  }
}

interface RenderJob {
  id: string
  userId: string
  avatarId: string
  voiceProfileId?: string
  status: 'pending' | 'queued' | 'processing' | 'completed' | 'failed' | 'cancelled'
  quality: 'draft' | 'standard' | 'high' | 'ultra'
  resolution: '480p' | '720p' | '1080p' | '1440p' | '4k'
  progress: number
  startTime: number
  endTime?: number
  audioFile?: string
  text: string
  enableAudio2Face?: boolean
  enableRealTimeLipsync?: boolean
  enableRayTracing?: boolean
  cameraAngle?: string
  lightingPreset?: string
  backgroundType?: string
  estimatedDuration?: number
  actualDuration?: number
  outputVideo?: string
  outputThumbnail?: string
  outputMetadata?: any
  lipSyncAccuracy?: number
  renderTime?: number
  fileSizeBytes?: number
  processingLog?: any[]
  error?: string
  retryCount?: number
  processingCost?: number
  creditsUsed?: number
  audio2FaceSession?: Audio2FaceSession
}

interface HyperRealisticRenderOptions {
  resolution: '4K' | '8K'
  quality: 'cinematic' | 'hyperreal'
  rayTracing: boolean
  realTimeLipSync: boolean
  audio2FaceEnabled: boolean
  voiceCloning: boolean
  language: 'pt-BR' | 'en-US' | 'es-ES'
}

export class Avatar3DHyperRealisticPipeline {
  private avatars: Map<string, Avatar3D> = new Map()
  private loadedScenes: Map<string, Avatar3DScene> = new Map()
  private renderQueue: Map<string, RenderJob> = new Map()
  private isAudio2FaceInitialized = false
  private supabase: ReturnType<typeof createClient<Database>>

  constructor() {
    this.supabase = createClient<Database>(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    )
    this.initializeAvatarLibrary()
    this.initializeAudio2Face()
  }

  private async initializeAudio2Face(): Promise<void> {
    try {
      console.log('üöÄ Inicializando Audio2Face integration...')
      this.isAudio2FaceInitialized = await audio2FaceService.checkHealth()
      if (this.isAudio2FaceInitialized) {
        console.log('‚úÖ Audio2Face integrado com sucesso!')
        
        // Registrar estat√≠sticas do sistema
        await this.updateSystemStats()
      } else {
        console.warn('‚ö†Ô∏è Audio2Face n√£o est√° dispon√≠vel, usando fallback')
      }
    } catch (error) {
      console.error('‚ùå Erro ao inicializar Audio2Face:', error)
      this.isAudio2FaceInitialized = false
    }
  }

  private initializeAvatarLibrary() {
    // Avatares 3D Hiper-Realistas para mercado brasileiro
    const hyperRealisticAvatars: Avatar3D[] = [
      {
        id: 'br_corporate_ana',
        name: 'Ana Paula - Executiva',
        category: 'business',
        gender: 'female',
        ethnicity: 'latino',
        age: 'adult',
        style: 'realistic',
        quality: 'hyperreal',
        features: {
          facialDetails: 'cinematic',
          skinTexture: 'photogrammetry',
          hairSystem: 'volumetric',
          clothingPhysics: true,
          microExpressions: true,
          eyeTracking: true,
          breathingAnimation: true,
          lipSyncAccuracy: 98
        },
        assets: {
          modelFile: '/avatars/3d/ana_corporate_8k.gltf',
          textureFiles: [
            '/avatars/textures/ana_diffuse_8k.jpg',
            '/avatars/textures/ana_normal_8k.jpg',
            '/avatars/textures/ana_roughness_4k.jpg',
            '/avatars/textures/ana_specular_4k.jpg'
          ],
          rigFile: '/avatars/rigs/ana_facial_rig.fbx',
          animationSet: [
            'idle_professional', 'talking_confident', 'gesture_presenting',
            'smile_warm', 'nod_agreement', 'point_screen'
          ],
          blendShapes: [
            'viseme_a', 'viseme_e', 'viseme_i', 'viseme_o', 'viseme_u',
            'smile', 'frown', 'blink', 'eyebrow_raise', 'jaw_open'
          ],
          materialFiles: [
            '/avatars/materials/skin_pbr.mat',
            '/avatars/materials/hair_strand.mat',
            '/avatars/materials/clothing_fabric.mat'
          ]
        },
        rendering: {
          resolution: '8K',
          antiAliasing: 'TAA',
          rayTracing: true,
          globalIllumination: true,
          subsurfaceScattering: true,
          volumetricLighting: true,
          motionBlur: true,
          depthOfField: true
        },
        animations: {
          idle: ['corporate_idle_01', 'corporate_idle_02', 'professional_stance'],
          talking: ['talk_confident', 'talk_explaining', 'talk_enthusiastic'],
          gestures: ['point_right', 'open_hands', 'thumbs_up', 'present_screen'],
          emotions: ['smile_warm', 'serious_focus', 'friendly_nod', 'confident_look'],
          transitions: ['idle_to_talk', 'talk_to_gesture', 'gesture_to_idle']
        },
        voiceSync: {
          supportedLanguages: ['pt-BR', 'en-US', 'es-ES'],
          phonemeMapping: {
            'a': 'viseme_a', 'e': 'viseme_e', 'i': 'viseme_i',
            'o': 'viseme_o', 'u': 'viseme_u'
          },
          lipSyncEngine: 'ml-driven',
          facialRig: 'advanced_facial_rig_v2'
        }
      },
      {
        id: 'br_safety_carlos',
        name: 'Carlos Silva - Seguran√ßa',
        category: 'safety',
        gender: 'male',
        ethnicity: 'mixed',
        age: 'adult',
        style: 'realistic',
        quality: 'hyperreal',
        features: {
          facialDetails: 'cinematic',
          skinTexture: 'photogrammetry',
          hairSystem: 'volumetric',
          clothingPhysics: true,
          microExpressions: true,
          eyeTracking: true,
          breathingAnimation: true,
          lipSyncAccuracy: 97
        },
        assets: {
          modelFile: '/avatars/3d/carlos_safety_8k.gltf',
          textureFiles: [
            '/avatars/textures/carlos_diffuse_8k.jpg',
            '/avatars/textures/carlos_normal_8k.jpg',
            '/avatars/textures/carlos_roughness_4k.jpg',
            '/avatars/textures/safety_helmet_4k.jpg'
          ],
          rigFile: '/avatars/rigs/carlos_safety_rig.fbx',
          animationSet: [
            'safety_check', 'demonstrate_epi', 'point_hazard',
            'authoritative_stance', 'safety_gesture', 'helmet_adjust'
          ],
          blendShapes: [
            'viseme_a', 'viseme_e', 'serious_look', 'safety_concern',
            'professional_smile', 'instructional_focus'
          ],
          materialFiles: [
            '/avatars/materials/male_skin_pbr.mat',
            '/avatars/materials/safety_helmet.mat',
            '/avatars/materials/uniform_fabric.mat'
          ]
        },
        rendering: {
          resolution: '8K',
          antiAliasing: 'TAA',
          rayTracing: true,
          globalIllumination: true,
          subsurfaceScattering: true,
          volumetricLighting: true,
          motionBlur: true,
          depthOfField: true
        },
        animations: {
          idle: ['safety_idle', 'authoritative_stance', 'observant_look'],
          talking: ['safety_instruction', 'explain_procedure', 'serious_talk'],
          gestures: ['point_equipment', 'stop_gesture', 'safety_check', 'demonstrate'],
          emotions: ['serious_concern', 'professional_smile', 'instructional', 'authoritative'],
          transitions: ['idle_to_instruction', 'instruction_to_demo', 'demo_to_idle']
        },
        voiceSync: {
          supportedLanguages: ['pt-BR'],
          phonemeMapping: {
            'a': 'viseme_a', 'e': 'viseme_e', 'i': 'viseme_i',
            'o': 'viseme_o', 'u': 'viseme_u'
          },
          lipSyncEngine: 'ml-driven',
          facialRig: 'safety_instructor_rig'
        }
      },
      {
        id: 'br_healthcare_marina',
        name: 'Marina Santos - Sa√∫de',
        category: 'healthcare',
        gender: 'female',
        ethnicity: 'mixed',
        age: 'adult',
        style: 'realistic',
        quality: 'hyperreal',
        features: {
          facialDetails: 'cinematic',
          skinTexture: 'photogrammetry',
          hairSystem: 'volumetric',
          clothingPhysics: true,
          microExpressions: true,
          eyeTracking: true,
          breathingAnimation: true,
          lipSyncAccuracy: 99
        },
        assets: {
          modelFile: '/avatars/3d/marina_healthcare_8k.gltf',
          textureFiles: [
            '/avatars/textures/marina_diffuse_8k.jpg',
            '/avatars/textures/marina_normal_8k.jpg',
            '/avatars/textures/medical_uniform_4k.jpg',
            '/avatars/textures/stethoscope_4k.jpg'
          ],
          rigFile: '/avatars/rigs/marina_medical_rig.fbx',
          animationSet: [
            'medical_explain', 'caring_gesture', 'professional_demo',
            'reassuring_smile', 'check_patient', 'write_notes'
          ],
          blendShapes: [
            'caring_expression', 'professional_focus', 'gentle_smile',
            'concerned_look', 'instructional_mode', 'empathetic'
          ],
          materialFiles: [
            '/avatars/materials/medical_skin_pbr.mat',
            '/avatars/materials/medical_uniform.mat',
            '/avatars/materials/stethoscope_metal.mat'
          ]
        },
        rendering: {
          resolution: '8K',
          antiAliasing: 'TAA',
          rayTracing: true,
          globalIllumination: true,
          subsurfaceScattering: true,
          volumetricLighting: true,
          motionBlur: true,
          depthOfField: true
        },
        animations: {
          idle: ['medical_idle', 'caring_stance', 'professional_ready'],
          talking: ['medical_explanation', 'caring_instruction', 'professional_brief'],
          gestures: ['gentle_point', 'demonstrate_procedure', 'caring_touch', 'check_chart'],
          emotions: ['caring_smile', 'professional_focus', 'gentle_concern', 'reassuring'],
          transitions: ['idle_to_care', 'care_to_instruction', 'instruction_to_demo']
        },
        voiceSync: {
          supportedLanguages: ['pt-BR'],
          phonemeMapping: {
            'a': 'viseme_a', 'e': 'viseme_e', 'i': 'viseme_i',
            'o': 'viseme_o', 'u': 'viseme_u'
          },
          lipSyncEngine: 'ml-driven',
          facialRig: 'medical_professional_rig'
        }
      }
    ]

    hyperRealisticAvatars.forEach(avatar => {
      this.avatars.set(avatar.id, avatar)
    })
  }

  // Renderiza√ß√£o hiper-realista com Audio2Face
  async renderHyperRealisticAvatar(
    userId: string,
    avatarId: string, 
    text: string,
    audioFile?: string,
    voiceProfileId?: string,
    renderOptions?: Partial<HyperRealisticRenderOptions>
  ): Promise<{
    jobId: string
    videoUrl: string
    thumbnailUrl: string
    renderTime: number
    lipSyncAccuracy?: number
    quality: {
      polygonCount: number
      textureResolution: string
      lightingModel: string
      renderingEngine: string
    }
  }> {
    const avatar = this.avatars.get(avatarId)
    if (!avatar) {
      throw new Error(`Avatar ${avatarId} n√£o encontrado`)
    }

    const options: HyperRealisticRenderOptions = {
      resolution: '4K',
      quality: 'hyperreal',
      rayTracing: true,
      realTimeLipSync: true,
      audio2FaceEnabled: true,
      voiceCloning: false,
      language: 'pt-BR',
      ...renderOptions
    }

    // Criar job de renderiza√ß√£o no Supabase
    const { data: renderJobData, error: jobError } = await this.supabase
      .from('render_jobs')
      .insert({
        user_id: userId,
        avatar_model_id: avatarId,
        voice_profile_id: voiceProfileId,
        script_text: text,
        audio_file_url: audioFile,
        status: 'pending',
        quality: options.quality === 'hyperreal' ? 'ultra' : 'high',
        resolution: options.resolution === '4K' ? '1080p' : '720p',
        enable_audio2face: options.audio2FaceEnabled,
        enable_real_time_lipsync: options.realTimeLipSync,
        enable_ray_tracing: options.rayTracing,
        progress_percentage: 0,
      })
      .select()
      .single()

    if (jobError || !renderJobData) {
      throw new Error(`Erro ao criar job de renderiza√ß√£o: ${jobError?.message}`)
    }

    const jobId = renderJobData.id
    const renderJob: RenderJob = {
      id: jobId,
      userId,
      avatarId,
      voiceProfileId,
      status: 'pending',
      quality: renderJobData.quality as any,
      resolution: renderJobData.resolution as any,
      progress: 0,
      startTime: Date.now(),
      audioFile,
      text,
      enableAudio2Face: options.audio2FaceEnabled,
      enableRealTimeLipsync: options.realTimeLipSync,
      enableRayTracing: options.rayTracing,
    }
    
    this.renderQueue.set(jobId, renderJob)

    try {
      // Atualizar status
      renderJob.status = 'processing'
      renderJob.progress = 10
      
      console.log(`üé≠ Renderizando avatar hiper-realista: ${avatar.name}`)
      console.log(`üìê Resolu√ß√£o: ${options.resolution}`)
      console.log(`‚ú® Qualidade: ${options.quality}`)
      console.log(`üî• Ray Tracing: ${options.rayTracing ? 'Ativado' : 'Desativado'}`)
      console.log(`üó£Ô∏è Audio2Face: ${options.audio2FaceEnabled ? 'Ativado' : 'Desativado'}`)

      let lipSyncAccuracy: number | undefined
      let audio2FaceSession: Audio2FaceSession | undefined
      
      // Processar lip-sync com Audio2Face se habilitado
      if (options.audio2FaceEnabled && audioFile && text && this.isAudio2FaceInitialized) {
        console.log('üé§ Processando lip-sync com Audio2Face...')
        
        // Atualizar status no Supabase
        await this.updateRenderJobProgress(jobId, 30, 'processing')
        renderJob.progress = 30
        renderJob.status = 'processing'
        
        try {
          // Criar sess√£o Audio2Face
          const audio2FaceConfig: Audio2FaceConfig = {
            ...defaultAudio2FaceConfig,
            qualityPreset: options.quality === 'hyperreal' ? 'ultra' : 'high',
            enableRealTime: options.realTimeLipSync,
          }
          
          audio2FaceSession = await audio2FaceService.createSession(jobId, audio2FaceConfig)
          renderJob.audio2FaceSession = audio2FaceSession
          
          // Processar √°udio (simulado - em produ√ß√£o seria o buffer real do √°udio)
          const audioBuffer = Buffer.from('mock_audio_data') // TODO: Carregar √°udio real
          const blendShapesData = await audio2FaceService.processAudio(
            audio2FaceSession.sessionId,
            audioBuffer,
            audio2FaceConfig
          )
          
          // Gerar curvas ARKit
          const arkitCurvesUrl = await audio2FaceService.generateARKitCurves(
            audio2FaceSession.sessionId,
            blendShapesData
          )
          
          lipSyncAccuracy = blendShapesData.metadata.accuracy
          
          console.log(`‚úÖ Lip-sync gerado com ${(lipSyncAccuracy * 100).toFixed(1)}% de precis√£o`)
          
          // Validar precis√£o m√≠nima (95%)
          const validation = await audio2FaceService.validateLipSyncAccuracy(
            audio2FaceSession.sessionId,
            0.95
          )
          
          if (!validation.isValid) {
            console.warn(`‚ö†Ô∏è ${validation.message}`)
          }
          
        } catch (error) {
          console.error('‚ùå Erro no processamento Audio2Face:', error)
          // Continuar com fallback
          lipSyncAccuracy = avatar.features.lipSyncAccuracy / 100
        }
      }
      
      renderJob.progress = 60
      
      // Simular renderiza√ß√£o Unreal Engine 5
      console.log('üé¨ Renderizando com Unreal Engine 5...')
      const baseRenderTime = options.quality === 'hyperreal' ? 8000 : 5000
      const resolutionMultiplier = options.resolution === '8K' ? 2 : 1
      const rayTracingMultiplier = options.rayTracing ? 1.5 : 1
      const audio2FaceMultiplier = options.audio2FaceEnabled ? 1.2 : 1
      
      const totalRenderTime = baseRenderTime * resolutionMultiplier * rayTracingMultiplier * audio2FaceMultiplier
      
      // Simular progresso de renderiza√ß√£o
      for (let progress = 60; progress <= 90; progress += 10) {
        renderJob.progress = progress
        await new Promise(resolve => setTimeout(resolve, totalRenderTime / 40))
      }
      
      renderJob.progress = 100
      renderJob.status = 'completed'
      renderJob.endTime = Date.now()
      
      const videoUrl = `/api/v2/avatars/render/${jobId}/video.mp4`
      const thumbnailUrl = `/api/v2/avatars/render/${jobId}/thumbnail.jpg`
      const renderTimeMs = renderJob.endTime - renderJob.startTime
      
      renderJob.outputVideo = videoUrl
      renderJob.outputThumbnail = thumbnailUrl
      renderJob.renderTime = renderTimeMs
      renderJob.lipSyncAccuracy = lipSyncAccuracy
      
      // Atualizar job no Supabase com resultados finais
      await this.supabase
        .from('render_jobs')
        .update({
          status: 'completed',
          progress_percentage: 100,
          output_video_url: videoUrl,
          output_thumbnail_url: thumbnailUrl,
          lipsync_accuracy: lipSyncAccuracy,
          render_time_seconds: renderTimeMs / 1000,
          actual_duration_seconds: renderTimeMs / 1000,
          output_metadata: {
            polygonCount: options.quality === 'hyperreal' ? 850000 : 450000,
            textureResolution: options.resolution === '8K' ? '8192x8192' : '4096x4096',
            lightingModel: 'Physically Based Rendering + Ray Tracing',
            renderingEngine: 'Unreal Engine 5 Lumen + Nanite + Audio2Face',
            audio2FaceEnabled: options.audio2FaceEnabled,
            lipSyncAccuracy: lipSyncAccuracy,
          },
          completed_at: new Date().toISOString(),
        })
        .eq('id', jobId)
      
      // Registrar analytics
      await this.recordAnalytics(userId, avatarId, jobId, 'render_completed', {
        renderTime: renderTimeMs,
        quality: options.quality,
        resolution: options.resolution,
        audio2FaceEnabled: options.audio2FaceEnabled,
        lipSyncAccuracy,
      })
      
      // Atualizar estat√≠sticas do sistema
      await this.updateSystemStats()
      
      console.log(`‚úÖ Renderiza√ß√£o conclu√≠da em ${renderTimeMs}ms`)
      
      return {
        jobId,
        videoUrl,
        thumbnailUrl,
        renderTime: renderTimeMs,
        lipSyncAccuracy,
        quality: {
          polygonCount: options.quality === 'hyperreal' ? 850000 : 450000,
          textureResolution: options.resolution === '8K' ? '8192x8192' : '4096x4096',
          lightingModel: 'Physically Based Rendering + Ray Tracing',
          renderingEngine: 'Unreal Engine 5 Lumen + Nanite + Audio2Face'
        }
      }
    } catch (error) {
      renderJob.status = 'failed'
      renderJob.error = error instanceof Error ? error.message : 'Erro desconhecido'
      renderJob.endTime = Date.now()
      
      // Atualizar status de erro no Supabase
      await this.supabase
        .from('render_jobs')
        .update({
          status: 'failed',
          error_message: renderJob.error,
          completed_at: new Date().toISOString(),
        })
        .eq('id', jobId)
      
      // Registrar analytics de erro
      await this.recordAnalytics(userId, avatarId, jobId, 'render_failed', {
        error: renderJob.error,
        renderTime: renderJob.endTime - renderJob.startTime,
      })
      
      throw error
    }
  }

  // Monitoramento de jobs de renderiza√ß√£o
  async getRenderJobStatus(jobId: string): Promise<RenderJob | null> {
    // Primeiro tentar da mem√≥ria
    const memoryJob = this.renderQueue.get(jobId)
    if (memoryJob) return memoryJob
    
    // Se n√£o estiver na mem√≥ria, buscar no banco
    return await this.getRenderJobFromDatabase(jobId)
  }

  async getAllRenderJobs(userId?: string): Promise<RenderJob[]> {
    try {
      let query = this.supabase
        .from('render_jobs')
        .select(`
          *,
          avatar_models(name, display_name),
          voice_profiles(name, display_name)
        `)
        .order('created_at', { ascending: false })
      
      if (userId) {
        query = query.eq('user_id', userId)
      }
      
      const { data, error } = await query
      
      if (error || !data) {
        return Array.from(this.renderQueue.values())
      }
      
      return data.map(job => ({
        id: job.id,
        userId: job.user_id,
        avatarId: job.avatar_model_id,
        voiceProfileId: job.voice_profile_id,
        status: job.status as any,
        quality: job.quality as any,
        resolution: job.resolution as any,
        progress: job.progress_percentage || 0,
        startTime: job.started_at ? new Date(job.started_at).getTime() : Date.now(),
        endTime: job.completed_at ? new Date(job.completed_at).getTime() : undefined,
        audioFile: job.audio_file_url,
        text: job.script_text,
        enableAudio2Face: job.enable_audio2face,
        enableRealTimeLipsync: job.enable_real_time_lipsync,
        enableRayTracing: job.enable_ray_tracing,
        cameraAngle: job.camera_angle,
        lightingPreset: job.lighting_preset,
        backgroundType: job.background_type,
        estimatedDuration: job.estimated_duration_seconds,
        actualDuration: job.actual_duration_seconds,
        outputVideo: job.output_video_url,
        outputThumbnail: job.output_thumbnail_url,
        outputMetadata: job.output_metadata,
        lipSyncAccuracy: job.lipsync_accuracy,
        renderTime: job.render_time_seconds ? job.render_time_seconds * 1000 : undefined,
        fileSizeBytes: job.file_size_bytes,
        processingLog: job.processing_log,
        error: job.error_message,
        retryCount: job.retry_count,
        processingCost: job.processing_cost,
        creditsUsed: job.credits_used,
      }))
    } catch (error) {
      console.error('Erro ao obter jobs:', error)
      return Array.from(this.renderQueue.values())
    }
  }

  async cancelRenderJob(jobId: string): Promise<boolean> {
    const job = this.renderQueue.get(jobId) || await this.getRenderJobFromDatabase(jobId)
    if (!job) return false

    if (job.status === 'processing' || job.status === 'queued') {
      try {
        // Atualizar no banco
        await this.supabase
          .from('render_jobs')
          .update({
            status: 'cancelled',
            completed_at: new Date().toISOString(),
            error_message: 'Job cancelado pelo usu√°rio'
          })
          .eq('id', jobId)
        
        // Atualizar na mem√≥ria
        if (this.renderQueue.has(jobId)) {
          const memoryJob = this.renderQueue.get(jobId)!
          memoryJob.status = 'cancelled'
          memoryJob.endTime = Date.now()
        }
        
        // Cancelar sess√£o Audio2Face se existir
        if (job.audio2FaceSession) {
          await audio2FaceService.cancelSession(job.audio2FaceSession.sessionId)
        }
        
        // Registrar analytics
        await this.recordAnalytics(
          job.userId,
          job.avatarId,
          jobId,
          'job_cancelled',
          { reason: 'user_request' }
        )
        
        console.log(`‚ùå Job ${jobId} cancelado`)
        return true
      } catch (error) {
        console.error('Erro ao cancelar job:', error)
        return false
      }
    }
    
    return false
  }

  // Lip sync hiper-realista com Audio2Face
  async generateHyperRealisticLipSync(
    avatarId: string,
    audioFile: string,
    text: string,
    options?: {
      language?: 'pt-BR' | 'en-US' | 'es-ES'
      quality?: 'standard' | 'high' | 'ultra'
      realTime?: boolean
    }
  ): Promise<{
    lipSyncData: Array<{
      time: number
      phoneme: string
      intensity: number
      blendShapes: Record<string, number>
    }>
    accuracy: number
    processingTime: number
    audio2FaceEnabled: boolean
  }> {
    const avatar = this.avatars.get(avatarId)
    if (!avatar) {
      throw new Error(`Avatar ${avatarId} n√£o encontrado`)
    }

    const config = {
      language: 'pt-BR',
      quality: 'high',
      realTime: false,
      ...options
    }

    console.log(`üó£Ô∏è Gerando lip sync hiper-realista para: ${avatar.name}`)
    console.log(`üåç Idioma: ${config.language}`)
    console.log(`‚ú® Qualidade: ${config.quality}`)
    
    const processingStart = Date.now()
    let lipSyncData: any[]
    let accuracy: number
    let audio2FaceEnabled = false

    // Tentar usar Audio2Face se dispon√≠vel
    if (this.isAudio2FaceInitialized) {
      try {
        console.log('üé§ Usando NVIDIA Audio2Face para lip-sync...')
        
        const lipSyncRequest: LipSyncRequest = {
          audioFile,
          text,
          avatarId,
          language: config.language,
          quality: config.quality,
          realTime: config.realTime
        }
        
        const audio2FaceResult = await audio2FaceService.generateARKitCurves(lipSyncRequest)
        
        // Converter ARKit curves para formato interno
        lipSyncData = audio2FaceResult.arkitCurves.map(curve => ({
          time: curve.timestamp,
          phoneme: this.arkitToPhoneme(curve.name),
          intensity: curve.weight,
          blendShapes: { [curve.name]: curve.weight }
        }))
        
        accuracy = audio2FaceResult.accuracy
        audio2FaceEnabled = true
        
        console.log(`‚úÖ Audio2Face processado com ${accuracy}% de precis√£o`)
      } catch (error) {
        console.warn('‚ö†Ô∏è Falha no Audio2Face, usando fallback:', error)
        // Fallback para m√©todo tradicional
        lipSyncData = this.generateMockLipSyncData(text, avatar)
        accuracy = avatar.features.lipSyncAccuracy
      }
    } else {
      console.log('üìù Usando m√©todo tradicional de lip-sync...')
      // M√©todo tradicional melhorado
      lipSyncData = this.generateMockLipSyncData(text, avatar)
      accuracy = avatar.features.lipSyncAccuracy
    }
    
    return {
      lipSyncData,
      accuracy,
      processingTime: Date.now() - processingStart,
      audio2FaceEnabled
    }
  }

  private generateMockLipSyncData(text: string, avatar: Avatar3D): any[] {
    const words = text.split(' ')
    const data: any[] = []
    let currentTime = 0
    
    words.forEach((word, index) => {
      const wordDuration = word.length * 0.08 // 80ms por caractere
      
      // Gerar visemes para cada fonema
      for (let i = 0; i < word.length; i++) {
        const char = word[i].toLowerCase()
        const phoneme = this.getPhoneme(char)
        const intensity = 0.7 + (Math.random() * 0.3) // 70-100% intensity
        
        data.push({
          time: currentTime + (i * wordDuration / word.length),
          phoneme,
          intensity,
          blendShapes: this.getBlendShapeWeights(phoneme, intensity)
        })
      }
      
      currentTime += wordDuration + 0.1 // 100ms pause entre palavras
    })
    
    return data
  }

  private getPhoneme(char: string): string {
    const phonemeMap: Record<string, string> = {
      'a': 'viseme_a', 'e': 'viseme_e', 'i': 'viseme_i',
      'o': 'viseme_o', 'u': 'viseme_u',
      'b': 'viseme_b', 'f': 'viseme_f', 'v': 'viseme_v',
      'm': 'viseme_m', 'p': 'viseme_p'
    }
    return phonemeMap[char] || 'viseme_rest'
  }

  private getBlendShapeWeights(phoneme: string, intensity: number): Record<string, number> {
    const baseWeights: Record<string, number> = {
      'jaw_open': 0,
      'mouth_close': 0,
      'lips_pucker': 0,
      'lips_stretch': 0,
      'tongue_up': 0
    }

    // Configurar blend shapes baseado no fonema
    switch (phoneme) {
      case 'viseme_a':
        baseWeights['jaw_open'] = intensity * 0.8
        baseWeights['mouth_close'] = 0
        break
      case 'viseme_o':
        baseWeights['lips_pucker'] = intensity * 0.9
        baseWeights['jaw_open'] = intensity * 0.3
        break
      case 'viseme_e':
        baseWeights['lips_stretch'] = intensity * 0.7
        baseWeights['jaw_open'] = intensity * 0.4
        break
      // ... mais fonemas
    }

    return baseWeights
  }

  // Obter avatares por categoria
  getAvatarsByCategory(category: string): Avatar3D[] {
    return Array.from(this.avatars.values())
      .filter(avatar => avatar.category === category)
      .sort((a, b) => {
        // Priorizar qualidade hiper-realista
        if (a.quality === 'hyperreal' && b.quality !== 'hyperreal') return -1
        if (b.quality === 'hyperreal' && a.quality !== 'hyperreal') return 1
        return a.name.localeCompare(b.name)
      })
  }

  // Todas as categorias dispon√≠veis
  getAllCategories(): Array<{
    id: string
    name: string
    description: string
    count: number
    preview: string
  }> {
    return [
      {
        id: 'business',
        name: 'Executivos',
        description: 'Profissionais corporativos hiper-realistas',
        count: this.getAvatarsByCategory('business').length,
        preview: 'br_corporate_ana'
      },
      {
        id: 'safety',
        name: 'Seguran√ßa',
        description: 'Instrutores de seguran√ßa do trabalho',
        count: this.getAvatarsByCategory('safety').length,
        preview: 'br_safety_carlos'
      },
      {
        id: 'healthcare',
        name: 'Sa√∫de',
        description: 'Profissionais de sa√∫de especializados',
        count: this.getAvatarsByCategory('healthcare').length,
        preview: 'br_healthcare_marina'
      },
      {
        id: 'education',
        name: 'Educa√ß√£o',
        description: 'Professores e instrutores acad√™micos',
        count: this.getAvatarsByCategory('education').length,
        preview: 'br_education_professor'
      },
      {
        id: 'casual',
        name: 'Informais',
        description: 'Personagens casuais e amig√°veis',
        count: this.getAvatarsByCategory('casual').length,
        preview: 'br_casual_friendly'
      }
    ]
  }

  // Configurar ilumina√ß√£o cinematogr√°fica
  configureCinematicLighting(sceneType: 'corporate' | 'safety' | 'medical' | 'educational'): Avatar3DScene['lighting'] {
    const lightingPresets = {
      corporate: {
        keyLight: { intensity: 0.8, color: '#fff5e6', angle: 45 },
        fillLight: { intensity: 0.4, color: '#e6f3ff', angle: -30 },
        rimLight: { intensity: 0.6, color: '#fff', angle: 135 },
        ambient: { intensity: 0.2, color: '#f0f4f8' }
      },
      safety: {
        keyLight: { intensity: 0.9, color: '#fff', angle: 60 },
        fillLight: { intensity: 0.5, color: '#ffe6e6', angle: -45 },
        rimLight: { intensity: 0.7, color: '#ffcc00', angle: 120 },
        ambient: { intensity: 0.25, color: '#f5f5f5' }
      },
      medical: {
        keyLight: { intensity: 0.85, color: '#f8f9fa', angle: 30 },
        fillLight: { intensity: 0.45, color: '#e3f2fd', angle: -60 },
        rimLight: { intensity: 0.5, color: '#e8f5e8', angle: 150 },
        ambient: { intensity: 0.3, color: '#fafafa' }
      },
      educational: {
        keyLight: { intensity: 0.75, color: '#fff9e6', angle: 40 },
        fillLight: { intensity: 0.4, color: '#e6f7ff', angle: -35 },
        rimLight: { intensity: 0.55, color: '#fff2e6', angle: 140 },
        ambient: { intensity: 0.25, color: '#f7f9fc' }
      }
    }

    return lightingPresets[sceneType]
  }

  // Obter avatar por ID
  getAvatar(avatarId: string): Avatar3D | null {
    return this.avatars.get(avatarId) || null
  }

  // Listar todos os avatares
  getAllAvatars(): Avatar3D[] {
    return Array.from(this.avatars.values())
  }

  // Converter ARKit blend shapes para fonemas
  private arkitToPhoneme(arkitName: string): string {
    const arkitToPhonemeMap: Record<string, string> = {
      'jawOpen': 'viseme_a',
      'mouthClose': 'viseme_rest',
      'mouthFunnel': 'viseme_o',
      'mouthPucker': 'viseme_u',
      'mouthSmileLeft': 'viseme_e',
      'mouthSmileRight': 'viseme_e',
      'mouthStretchLeft': 'viseme_i',
      'mouthStretchRight': 'viseme_i',
      'mouthLowerDownLeft': 'viseme_a',
      'mouthLowerDownRight': 'viseme_a'
    }
    return arkitToPhonemeMap[arkitName] || 'viseme_rest'
  }

  // Obter estat√≠sticas do pipeline
  async getPipelineStats() {
    try {
      // Obter estat√≠sticas do banco
      const { data: stats } = await this.supabase
        .from('system_stats')
        .select('*')
        .order('created_at', { ascending: false })
        .limit(1)
        .single()
      
      // Obter estat√≠sticas em tempo real da mem√≥ria
      const jobs = Array.from(this.renderQueue.values())
      const processing = jobs.filter(j => j.status === 'processing').length
      const queued = jobs.filter(j => j.status === 'queued').length
      
      // Combinar dados do banco com dados em tempo real
      return {
        totalJobs: stats?.total_renders || 0,
        totalAvatars: this.hyperRealisticAvatars.length,
        completed: stats?.completed_jobs || 0,
        failed: stats?.failed_jobs || 0,
        processing: processing,
        queued: queued,
        activeJobs: stats?.active_jobs || processing,
        successRate: stats?.success_rate || 0,
        avgRenderTime: Math.round(stats?.avg_render_time_seconds || 0),
        avgLipSyncAccuracy: Math.round((stats?.avg_lipsync_accuracy || 0) * 100) / 100,
        audio2FaceStatus: this.isAudio2FaceInitialized,
        redisStatus: stats?.redis_status || 'unknown',
        databaseStatus: stats?.database_status || 'unknown',
        availableAvatars: this.hyperRealisticAvatars.length,
        memoryUsage: process.memoryUsage(),
        uptime: Date.now() - this.startTime,
        lastUpdated: stats?.created_at || new Date().toISOString()
      }
    } catch (error) {
      console.error('Erro ao obter estat√≠sticas:', error)
      
      // Fallback para estat√≠sticas da mem√≥ria
      const jobs = Array.from(this.renderQueue.values())
      const completed = jobs.filter(j => j.status === 'completed').length
      const failed = jobs.filter(j => j.status === 'failed').length
      const processing = jobs.filter(j => j.status === 'processing').length
      const queued = jobs.filter(j => j.status === 'queued').length
      
      return {
        totalJobs: jobs.length,
        totalAvatars: this.hyperRealisticAvatars.length,
        completed,
        failed,
        processing,
        queued,
        activeJobs: processing,
        successRate: jobs.length > 0 ? completed / jobs.length : 0,
        avgRenderTime: 0,
        avgLipSyncAccuracy: 0,
        audio2FaceStatus: this.isAudio2FaceInitialized,
        redisStatus: 'unknown',
        databaseStatus: 'error',
        availableAvatars: this.hyperRealisticAvatars.length,
        memoryUsage: process.memoryUsage(),
        uptime: Date.now() - this.startTime,
        lastUpdated: new Date().toISOString()
      }
    }
  }

  // Limpar jobs antigos
  async cleanupOldJobs(maxAge: number = 24 * 60 * 60 * 1000): Promise<number> { // 24 horas por padr√£o
    try {
      const now = Date.now()
      let cleaned = 0
      
      // Limpar da mem√≥ria
      for (const [jobId, job] of this.renderQueue.entries()) {
        if (job.status === 'completed' || job.status === 'failed') {
          const jobAge = now - job.startTime
          if (jobAge > maxAge) {
            this.renderQueue.delete(jobId)
            cleaned++
          }
        }
      }
      
      // Limpar jobs antigos do banco (mais de 30 dias)
      const thirtyDaysAgo = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000).toISOString()
      
      const { error } = await this.supabase
        .from('render_jobs')
        .delete()
        .lt('completed_at', thirtyDaysAgo)
        .in('status', ['completed', 'failed', 'cancelled'])
      
      if (error) {
        console.error('Erro ao limpar jobs antigos do banco:', error)
      } else {
        console.log('üßπ Jobs antigos removidos do banco de dados')
      }
      
      // Limpar sess√µes antigas do Audio2Face
      if (this.isAudio2FaceInitialized) {
        await audio2FaceService.cleanupOldSessions()
      }
      
      return cleaned
    } catch (error) {
      console.error('Erro na limpeza de jobs antigos:', error)
      return 0
    }
  }

  // M√©todos auxiliares para Supabase
  private async updateRenderJobProgress(
    jobId: string, 
    progress: number, 
    status?: 'pending' | 'queued' | 'processing' | 'completed' | 'failed' | 'cancelled'
  ): Promise<void> {
    try {
      const updateData: any = { progress_percentage: progress }
      if (status) {
        updateData.status = status
        if (status === 'processing') {
          updateData.started_at = new Date().toISOString()
        }
      }
      
      await this.supabase
        .from('render_jobs')
        .update(updateData)
        .eq('id', jobId)
    } catch (error) {
      console.error('Erro ao atualizar progresso do job:', error)
    }
  }

  private async recordAnalytics(
    userId: string,
    avatarId: string,
    renderJobId: string,
    eventType: string,
    eventData: any
  ): Promise<void> {
    try {
      await this.supabase
        .from('avatar_analytics')
        .insert({
          user_id: userId,
          avatar_model_id: avatarId,
          render_job_id: renderJobId,
          event_type: eventType,
          event_data: eventData,
        })
    } catch (error) {
      console.error('Erro ao registrar analytics:', error)
    }
  }

  private async updateSystemStats(): Promise<void> {
    try {
      // Obter estat√≠sticas atuais
      const { data: jobs } = await this.supabase
        .from('render_jobs')
        .select('status, render_time_seconds, lipsync_accuracy')
      
      if (!jobs) return

      const totalRenders = jobs.length
      const activeJobs = jobs.filter(job => job.status === 'processing').length
      const completedJobs = jobs.filter(job => job.status === 'completed').length
      const failedJobs = jobs.filter(job => job.status === 'failed').length
      
      const completedJobsData = jobs.filter(job => 
        job.status === 'completed' && job.render_time_seconds
      )
      
      const avgRenderTime = completedJobsData.length > 0
        ? completedJobsData.reduce((sum, job) => sum + (job.render_time_seconds || 0), 0) / completedJobsData.length
        : 0
      
      const avgLipSyncAccuracy = completedJobsData.length > 0
        ? completedJobsData.reduce((sum, job) => sum + (job.lipsync_accuracy || 0), 0) / completedJobsData.length
        : 0
      
      const successRate = totalRenders > 0 ? completedJobs / totalRenders : 0

      // Inserir estat√≠sticas
      await this.supabase
        .from('system_stats')
        .insert({
          total_renders: totalRenders,
          active_jobs: activeJobs,
          completed_jobs: completedJobs,
          failed_jobs: failedJobs,
          avg_render_time_seconds: avgRenderTime,
          avg_lipsync_accuracy: avgLipSyncAccuracy,
          success_rate: successRate,
          audio2face_status: this.isAudio2FaceInitialized ? 'online' : 'offline',
          redis_status: 'online', // TODO: Verificar Redis real
          database_status: 'online',
        })
    } catch (error) {
      console.error('Erro ao atualizar estat√≠sticas do sistema:', error)
    }
  }

  // Obter jobs do Supabase
  async getRenderJobFromDatabase(jobId: string): Promise<RenderJob | null> {
    try {
      const { data, error } = await this.supabase
        .from('render_jobs')
        .select(`
          *,
          avatar_models(name, display_name),
          voice_profiles(name, display_name)
        `)
        .eq('id', jobId)
        .single()

      if (error || !data) {
        return null
      }

      return {
        id: data.id,
        userId: data.user_id,
        avatarId: data.avatar_model_id,
        voiceProfileId: data.voice_profile_id,
        status: data.status as any,
        quality: data.quality as any,
        resolution: data.resolution as any,
        progress: data.progress_percentage || 0,
        startTime: data.started_at ? new Date(data.started_at).getTime() : Date.now(),
        endTime: data.completed_at ? new Date(data.completed_at).getTime() : undefined,
        audioFile: data.audio_file_url,
        text: data.script_text,
        enableAudio2Face: data.enable_audio2face,
        enableRealTimeLipsync: data.enable_real_time_lipsync,
        enableRayTracing: data.enable_ray_tracing,
        cameraAngle: data.camera_angle,
        lightingPreset: data.lighting_preset,
        backgroundType: data.background_type,
        estimatedDuration: data.estimated_duration_seconds,
        actualDuration: data.actual_duration_seconds,
        outputVideo: data.output_video_url,
        outputThumbnail: data.output_thumbnail_url,
        outputMetadata: data.output_metadata,
        lipSyncAccuracy: data.lipsync_accuracy,
        renderTime: data.render_time_seconds ? data.render_time_seconds * 1000 : undefined,
        fileSizeBytes: data.file_size_bytes,
        processingLog: data.processing_log,
        error: data.error_message,
        retryCount: data.retry_count,
        processingCost: data.processing_cost,
        creditsUsed: data.credits_used,
      }
    } catch (error) {
      console.error('Erro ao obter job do banco:', error)
      return null
    }
  }

  // Shutdown do pipeline
  async shutdown(): Promise<void> {
    console.log('üõë Parando Avatar 3D Pipeline...')
    
    // Cancelar jobs ativos
    for (const job of this.renderQueue.values()) {
      if (job.status === 'processing') {
        this.cancelRenderJob(job.id)
      }
    }
    
    // Limpar sess√µes antigas do Audio2Face
    if (this.isAudio2FaceInitialized) {
      await audio2FaceService.cleanupOldSessions()
      this.isAudio2FaceInitialized = false
    }
    
    console.log('‚úÖ Pipeline parado com sucesso')
  }
}

export const avatar3DHyperPipeline = new Avatar3DHyperRealisticPipeline()

// Inst√¢ncia global do pipeline
export const avatar3DPipeline = new Avatar3DHyperRealisticPipeline()

// Inicializar o pipeline automaticamente
avatar3DPipeline.initializeAudio2Face().catch(error => {
  console.error('Erro ao inicializar Audio2Face:', error)
})
