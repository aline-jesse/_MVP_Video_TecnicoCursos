/**
 * üé≠ Advanced Lip-Sync Processor - Fase 1
 * Sistema avan√ßado de an√°lise de √°udio e sincroniza√ß√£o labial
 * An√°lise MFCC + Detec√ß√£o de Fonemas + Mapeamento para Visemas
 * Precis√£o alvo: 95%+
 */

import { MonitoringService } from '../monitoring-service'

// =====================================================
// INTERFACES E TIPOS
// =====================================================

export interface LipSyncConfig {
  // Configura√ß√µes de an√°lise
  sample_rate?: number // 16000, 22050, 44100, 48000
  frame_size?: number // Tamanho da janela em ms (25ms padr√£o)
  hop_length?: number // Sobreposi√ß√£o em ms (10ms padr√£o)
  
  // Configura√ß√µes MFCC
  n_mfcc?: number // N√∫mero de coeficientes MFCC (13 padr√£o)
  n_fft?: number // Tamanho da FFT (512 padr√£o)
  n_mels?: number // N√∫mero de filtros mel (40 padr√£o)
  
  // Configura√ß√µes de detec√ß√£o
  phoneme_threshold?: number // Limiar de confian√ßa para fonemas (0.7 padr√£o)
  viseme_smoothing?: number // Suaviza√ß√£o temporal (0.3 padr√£o)
  emotion_detection?: boolean // Detectar emo√ß√µes no √°udio
  
  // Configura√ß√µes de qualidade
  precision_mode?: 'fast' | 'balanced' | 'high' | 'ultra'
  enable_micro_expressions?: boolean
  enable_breathing_detection?: boolean
  
  // Configura√ß√µes de sa√≠da
  target_fps?: number // FPS do v√≠deo final (30 padr√£o)
  interpolation_method?: 'linear' | 'cubic' | 'bezier'
}

export interface LipSyncResult {
  // Identifica√ß√£o
  job_id: string
  audio_duration: number // segundos
  processing_time: number // ms
  
  // Dados principais
  visemes: VisemeFrame[]
  phonemes: PhonemeSegment[]
  blend_shapes: BlendShapeFrame[]
  
  // An√°lise avan√ßada
  emotion_timeline?: EmotionFrame[]
  breathing_events?: BreathingEvent[]
  micro_expressions?: MicroExpressionEvent[]
  
  // M√©tricas de qualidade
  confidence_score: number // 0.0 - 1.0
  accuracy_estimate: number // 0.0 - 1.0
  temporal_consistency: number // 0.0 - 1.0
  
  // Metadados
  mfcc_features: MFCCFeatures
  audio_analysis: AudioAnalysis
  processing_stats: ProcessingStats
  
  success: boolean
  error_message?: string
}

export interface VisemeFrame {
  timestamp: number // segundos
  viseme: VisemeType
  intensity: number // 0.0 - 1.0
  confidence: number // 0.0 - 1.0
  duration: number // segundos
  transition_type?: 'smooth' | 'sharp' | 'hold'
}

export interface PhonemeSegment {
  phoneme: string // IPA ou ARPABET
  start_time: number
  end_time: number
  confidence: number
  stress_level?: number // 0.0 - 1.0
  pitch_contour?: number[] // Contorno de pitch
}

export interface BlendShapeFrame {
  timestamp: number
  blend_shapes: Record<BlendShapeTarget, number> // 0.0 - 1.0
  jaw_open: number
  tongue_position: { x: number, y: number, z: number }
  lip_compression: number
}

export interface EmotionFrame {
  timestamp: number
  emotion: EmotionType
  intensity: number // 0.0 - 1.0
  confidence: number
  valence: number // -1.0 a 1.0 (negativo/positivo)
  arousal: number // 0.0 - 1.0 (calmo/excitado)
}

export interface BreathingEvent {
  timestamp: number
  type: 'inhale' | 'exhale' | 'pause'
  intensity: number
  duration: number
}

export interface MicroExpressionEvent {
  timestamp: number
  type: 'eyebrow_raise' | 'eye_squint' | 'nose_wrinkle' | 'lip_corner_pull'
  intensity: number
  duration: number
}

export interface MFCCFeatures {
  frames: number[][]
  frame_times: number[]
  feature_stats: {
    mean: number[]
    std: number[]
    min: number[]
    max: number[]
  }
}

export interface AudioAnalysis {
  rms_energy: number[]
  spectral_centroid: number[]
  spectral_rolloff: number[]
  zero_crossing_rate: number[]
  tempo: number
  key: string
  loudness: number // LUFS
}

export interface ProcessingStats {
  mfcc_extraction_time: number
  phoneme_detection_time: number
  viseme_mapping_time: number
  smoothing_time: number
  total_frames_processed: number
  memory_usage: number // MB
}

// =====================================================
// ENUMS E CONSTANTES
// =====================================================

export enum VisemeType {
  // Vogais
  A = 'A',      // /a/ - boca aberta
  E = 'E',      // /e/ - boca semi-aberta
  I = 'I',      // /i/ - boca fechada, l√°bios esticados
  O = 'O',      // /o/ - boca arredondada
  U = 'U',      // /u/ - boca muito arredondada
  
  // Consoantes labiais
  M = 'M',      // /m/, /b/, /p/ - l√°bios fechados
  F = 'F',      // /f/, /v/ - l√°bio inferior toca dentes superiores
  
  // Consoantes dentais/alveolares
  T = 'T',      // /t/, /d/, /s/, /z/ - l√≠ngua toca dentes/alv√©olo
  L = 'L',      // /l/ - l√≠ngua lateral
  R = 'R',      // /r/ - l√≠ngua vibra ou aproxima
  
  // Consoantes velares
  K = 'K',      // /k/, /g/ - l√≠ngua toca v√©u palatino
  
  // Especiais
  REST = 'REST', // Sil√™ncio
  TRANSITION = 'TRANSITION' // Transi√ß√£o entre visemas
}

export enum BlendShapeTarget {
  // Boca
  JAW_OPEN = 'jawOpen',
  MOUTH_SMILE_LEFT = 'mouthSmileLeft',
  MOUTH_SMILE_RIGHT = 'mouthSmileRight',
  MOUTH_FROWN_LEFT = 'mouthFrownLeft',
  MOUTH_FROWN_RIGHT = 'mouthFrownRight',
  MOUTH_PUCKER = 'mouthPucker',
  MOUTH_STRETCH = 'mouthStretch',
  MOUTH_ROLL_UPPER = 'mouthRollUpper',
  MOUTH_ROLL_LOWER = 'mouthRollLower',
  
  // L√°bios
  MOUTH_UPPER_UP_LEFT = 'mouthUpperUpLeft',
  MOUTH_UPPER_UP_RIGHT = 'mouthUpperUpRight',
  MOUTH_LOWER_DOWN_LEFT = 'mouthLowerDownLeft',
  MOUTH_LOWER_DOWN_RIGHT = 'mouthLowerDownRight',
  
  // Bochechas
  CHEEK_PUFF = 'cheekPuff',
  CHEEK_SQUINT_LEFT = 'cheekSquintLeft',
  CHEEK_SQUINT_RIGHT = 'cheekSquintRight',
  
  // L√≠ngua (se suportado)
  TONGUE_OUT = 'tongueOut'
}

export enum EmotionType {
  NEUTRAL = 'neutral',
  HAPPY = 'happy',
  SAD = 'sad',
  ANGRY = 'angry',
  SURPRISED = 'surprised',
  FEARFUL = 'fearful',
  DISGUSTED = 'disgusted',
  EXCITED = 'excited',
  CALM = 'calm'
}

// =====================================================
// MAPEAMENTOS DE FONEMAS PARA VISEMAS
// =====================================================

const PHONEME_TO_VISEME_MAP: Record<string, VisemeType> = {
  // Vogais portuguesas
  'a': VisemeType.A,
  '…ê': VisemeType.A,
  'e': VisemeType.E,
  '…õ': VisemeType.E,
  'i': VisemeType.I,
  'o': VisemeType.O,
  '…î': VisemeType.O,
  'u': VisemeType.U,
  
  // Consoantes labiais
  'p': VisemeType.M,
  'b': VisemeType.M,
  'm': VisemeType.M,
  'f': VisemeType.F,
  'v': VisemeType.F,
  
  // Consoantes dentais/alveolares
  't': VisemeType.T,
  'd': VisemeType.T,
  's': VisemeType.T,
  'z': VisemeType.T,
  'n': VisemeType.T,
  'l': VisemeType.L,
  '…æ': VisemeType.R,
  'r': VisemeType.R,
  
  // Consoantes velares
  'k': VisemeType.K,
  'g': VisemeType.K,
  '≈ã': VisemeType.K,
  
  // Africadas e fricativas
  ' É': VisemeType.T,
  ' í': VisemeType.T,
  't É': VisemeType.T,
  'd í': VisemeType.T,
  
  // Semivogais
  'j': VisemeType.I,
  'w': VisemeType.U,
  
  // Sil√™ncio
  'sil': VisemeType.REST,
  'sp': VisemeType.REST
}

// =====================================================
// CONFIGURA√á√ïES PADR√ÉO
// =====================================================

const DEFAULT_CONFIG: Required<LipSyncConfig> = {
  sample_rate: 22050,
  frame_size: 25, // ms
  hop_length: 10, // ms
  n_mfcc: 13,
  n_fft: 512,
  n_mels: 40,
  phoneme_threshold: 0.7,
  viseme_smoothing: 0.3,
  emotion_detection: true,
  precision_mode: 'balanced',
  enable_micro_expressions: true,
  enable_breathing_detection: true,
  target_fps: 30,
  interpolation_method: 'cubic'
}

// =====================================================
// ADVANCED LIP-SYNC PROCESSOR
// =====================================================

export class AdvancedLipSyncProcessor {
  private static instance: AdvancedLipSyncProcessor
  private monitoring: MonitoringService

  private constructor() {
    this.monitoring = MonitoringService.getInstance()
  }

  static getInstance(): AdvancedLipSyncProcessor {
    if (!AdvancedLipSyncProcessor.instance) {
      AdvancedLipSyncProcessor.instance = new AdvancedLipSyncProcessor()
    }
    return AdvancedLipSyncProcessor.instance
  }

  // =====================================================
  // M√âTODO PRINCIPAL
  // =====================================================

  async processAudio(
    audioBuffer: Buffer,
    config: LipSyncConfig = {},
    jobId?: string
  ): Promise<LipSyncResult> {
    const startTime = Date.now()
    const finalJobId = jobId || this.generateJobId()
    const finalConfig = { ...DEFAULT_CONFIG, ...config }

    this.monitoring.log('info', 'lipsync_start', `Iniciando processamento lip-sync`, {
      jobId: finalJobId,
      audioSize: audioBuffer.length,
      config: finalConfig
    })

    try {
      // 1. An√°lise inicial do √°udio
      const audioAnalysis = await this.analyzeAudio(audioBuffer, finalConfig)
      
      // 2. Extra√ß√£o de caracter√≠sticas MFCC
      const mfccFeatures = await this.extractMFCCFeatures(audioBuffer, finalConfig)
      
      // 3. Detec√ß√£o de fonemas
      const phonemes = await this.detectPhonemes(audioBuffer, mfccFeatures, finalConfig)
      
      // 4. Mapeamento para visemas
  const visemes = await this.mapPhonemesToVisemes(phonemes, finalConfig)
      
      // 5. Gera√ß√£o de blend shapes
      const blendShapes = await this.generateBlendShapes(visemes, finalConfig)
      
      // 6. An√°lise de emo√ß√µes (se habilitada)
      const emotionTimeline = finalConfig.emotion_detection 
        ? await this.detectEmotions(audioBuffer, mfccFeatures, finalConfig)
        : undefined
      
      // 7. Detec√ß√£o de respira√ß√£o (se habilitada)
      const breathingEvents = finalConfig.enable_breathing_detection
        ? await this.detectBreathing(audioBuffer, audioAnalysis, finalConfig)
        : undefined
      
      // 8. Micro-express√µes (se habilitadas)
      const microExpressions = finalConfig.enable_micro_expressions
        ? await this.detectMicroExpressions(audioBuffer, emotionTimeline, finalConfig)
        : undefined
      
      // 9. Suaviza√ß√£o temporal
      const smoothedVisemes = await this.applySmoothingFilter(visemes, finalConfig)
      const smoothedBlendShapes = await this.smoothBlendShapes(blendShapes, finalConfig)
      
      // 10. C√°lculo de m√©tricas de qualidade
      const qualityMetrics = await this.calculateQualityMetrics(
        smoothedVisemes,
        phonemes,
        audioAnalysis
      )

      const processingTime = Date.now() - startTime
      
      const result: LipSyncResult = {
        job_id: finalJobId,
        audio_duration: audioAnalysis.duration,
        processing_time: processingTime,
        visemes: smoothedVisemes,
        phonemes,
        blend_shapes: smoothedBlendShapes,
        emotion_timeline: emotionTimeline,
        breathing_events: breathingEvents,
        micro_expressions: microExpressions,
        confidence_score: qualityMetrics.confidence,
        accuracy_estimate: qualityMetrics.accuracy,
        temporal_consistency: qualityMetrics.consistency,
        mfcc_features: mfccFeatures,
        audio_analysis: audioAnalysis,
        processing_stats: {
          mfcc_extraction_time: 0, // Ser√° preenchido durante o processamento
          phoneme_detection_time: 0,
          viseme_mapping_time: 0,
          smoothing_time: 0,
          total_frames_processed: smoothedVisemes.length,
          memory_usage: process.memoryUsage().heapUsed / 1024 / 1024
        },
        success: true
      }

      this.monitoring.log('info', 'lipsync_success', `Lip-sync processado com sucesso`, {
        jobId: finalJobId,
        processingTime,
        confidenceScore: qualityMetrics.confidence,
        totalFrames: smoothedVisemes.length
      })

      return result

    } catch (error) {
      const processingTime = Date.now() - startTime
      
      this.monitoring.log('error', 'lipsync_error', `Erro no processamento lip-sync: ${error}`, {
        jobId: finalJobId,
        processingTime,
        error: error instanceof Error ? error.message : String(error)
      })

      throw error
    }
  }

  // =====================================================
  // AN√ÅLISE DE √ÅUDIO
  // =====================================================

  private async analyzeAudio(audioBuffer: Buffer, config: Required<LipSyncConfig>): Promise<AudioAnalysis> {
    // Simular an√°lise de √°udio b√°sica
    // TODO: Implementar an√°lise real com bibliotecas de processamento de √°udio
    
    const duration = audioBuffer.length / (config.sample_rate * 2) // Assumindo 16-bit
    const frameCount = Math.floor(duration * config.target_fps)
    
    return {
      duration,
      rms_energy: new Array(frameCount).fill(0).map(() => Math.random() * 0.5),
      spectral_centroid: new Array(frameCount).fill(0).map(() => 1000 + Math.random() * 2000),
      spectral_rolloff: new Array(frameCount).fill(0).map(() => 2000 + Math.random() * 3000),
      zero_crossing_rate: new Array(frameCount).fill(0).map(() => Math.random() * 0.1),
      tempo: 120 + Math.random() * 60,
      key: 'C',
      loudness: -23 + Math.random() * 10 // LUFS
    }
  }

  // =====================================================
  // EXTRA√á√ÉO MFCC
  // =====================================================

  private async extractMFCCFeatures(
    audioBuffer: Buffer,
    config: Required<LipSyncConfig>
  ): Promise<MFCCFeatures> {
    // TODO: Implementar extra√ß√£o MFCC real
    // Por enquanto, gerar dados simulados
    
    const duration = audioBuffer.length / (config.sample_rate * 2)
    const frameCount = Math.floor(duration / (config.hop_length / 1000))
    
    const frames: number[][] = []
    const frameTimes: number[] = []
    
    for (let i = 0; i < frameCount; i++) {
      const time = i * (config.hop_length / 1000)
      frameTimes.push(time)
      
      // Gerar coeficientes MFCC simulados
      const mfccFrame = new Array(config.n_mfcc).fill(0).map(() => 
        Math.random() * 20 - 10 // Valores t√≠picos de MFCC
      )
      frames.push(mfccFrame)
    }
    
    // Calcular estat√≠sticas
    const featureStats = this.calculateFeatureStatistics(frames)
    
    return {
      frames,
      frame_times: frameTimes,
      feature_stats: featureStats
    }
  }

  private calculateFeatureStatistics(frames: number[][]): MFCCFeatures['feature_stats'] {
    if (frames.length === 0) {
      return { mean: [], std: [], min: [], max: [] }
    }
    
    const numFeatures = frames[0].length
    const mean = new Array(numFeatures).fill(0)
    const std = new Array(numFeatures).fill(0)
    const min = new Array(numFeatures).fill(Infinity)
    const max = new Array(numFeatures).fill(-Infinity)
    
    // Calcular min, max e soma para m√©dia
    for (const frame of frames) {
      for (let i = 0; i < numFeatures; i++) {
        const value = frame[i]
        mean[i] += value
        min[i] = Math.min(min[i], value)
        max[i] = Math.max(max[i], value)
      }
    }
    
    // Finalizar m√©dia
    for (let i = 0; i < numFeatures; i++) {
      mean[i] /= frames.length
    }
    
    // Calcular desvio padr√£o
    for (const frame of frames) {
      for (let i = 0; i < numFeatures; i++) {
        const diff = frame[i] - mean[i]
        std[i] += diff * diff
      }
    }
    
    for (let i = 0; i < numFeatures; i++) {
      std[i] = Math.sqrt(std[i] / frames.length)
    }
    
    return { mean, std, min, max }
  }

  // =====================================================
  // DETEC√á√ÉO DE FONEMAS
  // =====================================================

  private async detectPhonemes(
    audioBuffer: Buffer,
    mfccFeatures: MFCCFeatures,
    config: Required<LipSyncConfig>
  ): Promise<PhonemeSegment[]> {
    // TODO: Implementar detec√ß√£o real de fonemas usando ML
    // Por enquanto, gerar segmentos simulados baseados em padr√µes t√≠picos
    
    const phonemes: PhonemeSegment[] = []
    const duration = mfccFeatures.frame_times[mfccFeatures.frame_times.length - 1] || 1
    
    // Fonemas comuns em portugu√™s brasileiro
    const commonPhonemes = ['a', 'e', 'i', 'o', 'u', 'p', 'b', 't', 'd', 'k', 'g', 'm', 'n', 'l', 'r', 's', 'z']
    
    let currentTime = 0
    while (currentTime < duration) {
      const phoneme = commonPhonemes[Math.floor(Math.random() * commonPhonemes.length)]
      const segmentDuration = 0.05 + Math.random() * 0.15 // 50-200ms
      const confidence = config.phoneme_threshold + Math.random() * (1 - config.phoneme_threshold)
      
      phonemes.push({
        phoneme,
        start_time: currentTime,
        end_time: currentTime + segmentDuration,
        confidence,
        stress_level: Math.random(),
        pitch_contour: this.generatePitchContour(segmentDuration)
      })
      
      currentTime += segmentDuration
    }
    
    return phonemes
  }

  private generatePitchContour(duration: number): number[] {
    const points = Math.max(3, Math.floor(duration * 100)) // 100 pontos por segundo
    const contour: number[] = []
    
    const basePitch = 150 + Math.random() * 100 // 150-250 Hz
    
    for (let i = 0; i < points; i++) {
      const variation = Math.sin(i / points * Math.PI * 2) * 20 // Varia√ß√£o de ¬±20 Hz
      contour.push(basePitch + variation)
    }
    
    return contour
  }

  // =====================================================
  // MAPEAMENTO PARA VISEMAS
  // =====================================================

  private async mapPhonemesToVisemes(
    phonemes: PhonemeSegment[],
    config: Required<LipSyncConfig>
  ): Promise<VisemeFrame[]> {
    const visemes: VisemeFrame[] = []
    
    for (const phoneme of phonemes) {
      const visemeType = PHONEME_TO_VISEME_MAP[phoneme.phoneme] || VisemeType.REST
      const duration = phoneme.end_time - phoneme.start_time
      
      // Determinar intensidade baseada na confian√ßa e stress
      const baseIntensity = phoneme.confidence * (phoneme.stress_level || 0.5)
      const intensity = Math.min(1.0, baseIntensity * (0.7 + Math.random() * 0.3))
      
      // Determinar tipo de transi√ß√£o
      const transitionType = this.determineTransitionType(visemeType, intensity)
      
      visemes.push({
        timestamp: phoneme.start_time,
        viseme: visemeType,
        intensity,
        confidence: phoneme.confidence,
        duration,
        transition_type: transitionType
      })
    }
    
    return visemes
  }

  private determineTransitionType(viseme: VisemeType, intensity: number): 'smooth' | 'sharp' | 'hold' {
    // Transi√ß√µes suaves para vogais
    if ([VisemeType.A, VisemeType.E, VisemeType.I, VisemeType.O, VisemeType.U].includes(viseme)) {
      return 'smooth'
    }
    
    // Transi√ß√µes n√≠tidas para consoantes explosivas
    if ([VisemeType.M, VisemeType.T, VisemeType.K].includes(viseme)) {
      return intensity > 0.7 ? 'sharp' : 'smooth'
    }
    
    // Manter para fricativas
    if ([VisemeType.F, VisemeType.L, VisemeType.R].includes(viseme)) {
      return 'hold'
    }
    
    return 'smooth'
  }

  // =====================================================
  // GERA√á√ÉO DE BLEND SHAPES
  // =====================================================

  private async generateBlendShapes(
    visemes: VisemeFrame[],
    config: Required<LipSyncConfig>
  ): Promise<BlendShapeFrame[]> {
    const blendShapes: BlendShapeFrame[] = []
    
    for (const viseme of visemes) {
      const blendShapeValues = this.visemeToBlendShapes(viseme)
      
      blendShapes.push({
        timestamp: viseme.timestamp,
        blend_shapes: blendShapeValues,
        jaw_open: this.calculateJawOpen(viseme),
        tongue_position: this.calculateTonguePosition(viseme),
        lip_compression: this.calculateLipCompression(viseme)
      })
    }
    
    return blendShapes
  }

  private visemeToBlendShapes(viseme: VisemeFrame): Record<BlendShapeTarget, number> {
    const intensity = viseme.intensity
    const blendShapes: Record<BlendShapeTarget, number> = {} as any
    
    // Inicializar todos os blend shapes com 0
    Object.values(BlendShapeTarget).forEach(target => {
      blendShapes[target] = 0
    })
    
    switch (viseme.viseme) {
      case VisemeType.A:
        blendShapes[BlendShapeTarget.JAW_OPEN] = intensity * 0.8
        blendShapes[BlendShapeTarget.MOUTH_STRETCH] = intensity * 0.3
        break
        
      case VisemeType.E:
        blendShapes[BlendShapeTarget.JAW_OPEN] = intensity * 0.4
        blendShapes[BlendShapeTarget.MOUTH_STRETCH] = intensity * 0.6
        break
        
      case VisemeType.I:
        blendShapes[BlendShapeTarget.MOUTH_STRETCH] = intensity * 0.8
        blendShapes[BlendShapeTarget.CHEEK_SQUINT_LEFT] = intensity * 0.2
        blendShapes[BlendShapeTarget.CHEEK_SQUINT_RIGHT] = intensity * 0.2
        break
        
      case VisemeType.O:
      case VisemeType.U:
        blendShapes[BlendShapeTarget.MOUTH_PUCKER] = intensity * 0.7
        blendShapes[BlendShapeTarget.JAW_OPEN] = intensity * 0.3
        break
        
      case VisemeType.M:
        blendShapes[BlendShapeTarget.MOUTH_ROLL_UPPER] = intensity * 0.8
        blendShapes[BlendShapeTarget.MOUTH_ROLL_LOWER] = intensity * 0.8
        break
        
      case VisemeType.F:
        blendShapes[BlendShapeTarget.MOUTH_LOWER_DOWN_LEFT] = intensity * 0.6
        blendShapes[BlendShapeTarget.MOUTH_LOWER_DOWN_RIGHT] = intensity * 0.6
        blendShapes[BlendShapeTarget.MOUTH_UPPER_UP_LEFT] = intensity * 0.4
        blendShapes[BlendShapeTarget.MOUTH_UPPER_UP_RIGHT] = intensity * 0.4
        break
        
      case VisemeType.T:
        blendShapes[BlendShapeTarget.JAW_OPEN] = intensity * 0.2
        blendShapes[BlendShapeTarget.TONGUE_OUT] = intensity * 0.3
        break
        
      case VisemeType.L:
        blendShapes[BlendShapeTarget.TONGUE_OUT] = intensity * 0.5
        blendShapes[BlendShapeTarget.JAW_OPEN] = intensity * 0.3
        break
        
      case VisemeType.R:
        blendShapes[BlendShapeTarget.MOUTH_PUCKER] = intensity * 0.4
        blendShapes[BlendShapeTarget.TONGUE_OUT] = intensity * 0.2
        break
        
      case VisemeType.K:
        blendShapes[BlendShapeTarget.JAW_OPEN] = intensity * 0.4
        break
        
      case VisemeType.REST:
      default:
        // Todos os valores j√° s√£o 0
        break
    }
    
    return blendShapes
  }

  private calculateJawOpen(viseme: VisemeFrame): number {
    const openVisemes = [VisemeType.A, VisemeType.O, VisemeType.U]
    if (openVisemes.includes(viseme.viseme)) {
      return viseme.intensity * 0.8
    }
    return viseme.intensity * 0.2
  }

  private calculateTonguePosition(viseme: VisemeFrame): { x: number, y: number, z: number } {
    const intensity = viseme.intensity
    
    switch (viseme.viseme) {
      case VisemeType.L:
      case VisemeType.T:
        return { x: 0, y: intensity * 0.5, z: intensity * 0.3 }
      case VisemeType.R:
        return { x: 0, y: intensity * 0.3, z: -intensity * 0.2 }
      case VisemeType.K:
        return { x: 0, y: -intensity * 0.4, z: -intensity * 0.5 }
      default:
        return { x: 0, y: 0, z: 0 }
    }
  }

  private calculateLipCompression(viseme: VisemeFrame): number {
    const compressedVisemes = [VisemeType.M, VisemeType.F]
    if (compressedVisemes.includes(viseme.viseme)) {
      return viseme.intensity * 0.9
    }
    return 0
  }

  // =====================================================
  // AN√ÅLISE DE EMO√á√ïES
  // =====================================================

  private async detectEmotions(
    audioBuffer: Buffer,
    mfccFeatures: MFCCFeatures,
    config: Required<LipSyncConfig>
  ): Promise<EmotionFrame[]> {
    // TODO: Implementar detec√ß√£o real de emo√ß√µes
    const emotions: EmotionFrame[] = []
    const duration = mfccFeatures.frame_times[mfccFeatures.frame_times.length - 1] || 1
    
    const emotionTypes = Object.values(EmotionType)
    let currentTime = 0
    
    while (currentTime < duration) {
      const emotion = emotionTypes[Math.floor(Math.random() * emotionTypes.length)]
      const segmentDuration = 1 + Math.random() * 3 // 1-4 segundos
      
      emotions.push({
        timestamp: currentTime,
        emotion,
        intensity: 0.3 + Math.random() * 0.7,
        confidence: 0.6 + Math.random() * 0.4,
        valence: Math.random() * 2 - 1, // -1 a 1
        arousal: Math.random()
      })
      
      currentTime += segmentDuration
    }
    
    return emotions
  }

  // =====================================================
  // DETEC√á√ÉO DE RESPIRA√á√ÉO
  // =====================================================

  private async detectBreathing(
    audioBuffer: Buffer,
    audioAnalysis: AudioAnalysis,
    config: Required<LipSyncConfig>
  ): Promise<BreathingEvent[]> {
    const breathingEvents: BreathingEvent[] = []
    
    // Detectar pausas na energia RMS como poss√≠veis respira√ß√µes
    for (let i = 0; i < audioAnalysis.rms_energy.length - 1; i++) {
      const currentEnergy = audioAnalysis.rms_energy[i]
      const nextEnergy = audioAnalysis.rms_energy[i + 1]
      
      // Detectar queda significativa na energia
      if (currentEnergy > 0.1 && nextEnergy < 0.05) {
        const timestamp = (i / audioAnalysis.rms_energy.length) * audioAnalysis.duration
        
        breathingEvents.push({
          timestamp,
          type: 'exhale',
          intensity: currentEnergy,
          duration: 0.2 + Math.random() * 0.3
        })
      }
      
      // Detectar aumento significativo na energia
      if (currentEnergy < 0.05 && nextEnergy > 0.1) {
        const timestamp = (i / audioAnalysis.rms_energy.length) * audioAnalysis.duration
        
        breathingEvents.push({
          timestamp,
          type: 'inhale',
          intensity: nextEnergy,
          duration: 0.1 + Math.random() * 0.2
        })
      }
    }
    
    return breathingEvents
  }

  // =====================================================
  // DETEC√á√ÉO DE MICRO-EXPRESS√ïES
  // =====================================================

  private async detectMicroExpressions(
    audioBuffer: Buffer,
    emotionTimeline: EmotionFrame[] | undefined,
    config: Required<LipSyncConfig>
  ): Promise<MicroExpressionEvent[]> {
    if (!emotionTimeline) return []
    
    const microExpressions: MicroExpressionEvent[] = []
    
    for (const emotion of emotionTimeline) {
      // Gerar micro-express√µes baseadas na emo√ß√£o
      if (emotion.intensity > 0.7) {
        const expressionTypes = this.getExpressionTypesForEmotion(emotion.emotion)
        
        for (const type of expressionTypes) {
          if (Math.random() < 0.3) { // 30% de chance
            microExpressions.push({
              timestamp: emotion.timestamp + Math.random() * 0.5,
              type,
              intensity: emotion.intensity * (0.5 + Math.random() * 0.5),
              duration: 0.1 + Math.random() * 0.2
            })
          }
        }
      }
    }
    
    return microExpressions
  }

  private getExpressionTypesForEmotion(emotion: EmotionType): MicroExpressionEvent['type'][] {
    switch (emotion) {
      case EmotionType.HAPPY:
      case EmotionType.EXCITED:
        return ['eyebrow_raise', 'lip_corner_pull']
      case EmotionType.SURPRISED:
        return ['eyebrow_raise', 'eye_squint']
      case EmotionType.ANGRY:
      case EmotionType.DISGUSTED:
        return ['nose_wrinkle', 'eye_squint']
      case EmotionType.SAD:
        return ['lip_corner_pull']
      default:
        return []
    }
  }

  // =====================================================
  // SUAVIZA√á√ÉO TEMPORAL
  // =====================================================

  private async applySmoothingFilter(
    visemes: VisemeFrame[],
    config: Required<LipSyncConfig>
  ): Promise<VisemeFrame[]> {
    if (visemes.length < 3) return visemes
    
    const smoothed = [...visemes]
    const smoothingFactor = config.viseme_smoothing
    
    // Aplicar filtro de m√©dia m√≥vel
    for (let i = 1; i < smoothed.length - 1; i++) {
      const prev = visemes[i - 1]
      const current = visemes[i]
      const next = visemes[i + 1]
      
      // Suavizar intensidade
      smoothed[i].intensity = 
        current.intensity * (1 - smoothingFactor) +
        (prev.intensity + next.intensity) * smoothingFactor / 2
      
      // Suavizar confian√ßa
      smoothed[i].confidence = 
        current.confidence * (1 - smoothingFactor) +
        (prev.confidence + next.confidence) * smoothingFactor / 2
    }
    
    return smoothed
  }

  private async smoothBlendShapes(
    blendShapes: BlendShapeFrame[],
    config: Required<LipSyncConfig>
  ): Promise<BlendShapeFrame[]> {
    if (blendShapes.length < 3) return blendShapes
    
    const smoothed = [...blendShapes]
    const smoothingFactor = config.viseme_smoothing
    
    for (let i = 1; i < smoothed.length - 1; i++) {
      const prev = blendShapes[i - 1]
      const current = blendShapes[i]
      const next = blendShapes[i + 1]
      
      // Suavizar cada blend shape
      Object.keys(current.blend_shapes).forEach(key => {
        const target = key as BlendShapeTarget
        smoothed[i].blend_shapes[target] = 
          current.blend_shapes[target] * (1 - smoothingFactor) +
          (prev.blend_shapes[target] + next.blend_shapes[target]) * smoothingFactor / 2
      })
      
      // Suavizar jaw_open
      smoothed[i].jaw_open = 
        current.jaw_open * (1 - smoothingFactor) +
        (prev.jaw_open + next.jaw_open) * smoothingFactor / 2
      
      // Suavizar lip_compression
      smoothed[i].lip_compression = 
        current.lip_compression * (1 - smoothingFactor) +
        (prev.lip_compression + next.lip_compression) * smoothingFactor / 2
    }
    
    return smoothed
  }

  // =====================================================
  // M√âTRICAS DE QUALIDADE
  // =====================================================

  private async calculateQualityMetrics(
    visemes: VisemeFrame[],
    phonemes: PhonemeSegment[],
    audioAnalysis: AudioAnalysis
  ): Promise<{ confidence: number, accuracy: number, consistency: number }> {
    
    // Calcular confian√ßa m√©dia
    const avgConfidence = visemes.reduce((sum, v) => sum + v.confidence, 0) / visemes.length
    
    // Estimar precis√£o baseada na correspond√™ncia fonema-visema
    const phonemeVisemeAlignment = this.calculatePhonemeVisemeAlignment(phonemes, visemes)
    
    // Calcular consist√™ncia temporal
    const temporalConsistency = this.calculateTemporalConsistency(visemes)
    
    return {
      confidence: avgConfidence,
      accuracy: phonemeVisemeAlignment,
      consistency: temporalConsistency
    }
  }

  private calculatePhonemeVisemeAlignment(
    phonemes: PhonemeSegment[],
    visemes: VisemeFrame[]
  ): number {
    if (phonemes.length === 0 || visemes.length === 0) return 0
    
    let alignedCount = 0
    
    for (const phoneme of phonemes) {
      // Encontrar visema correspondente no tempo
      const correspondingViseme = visemes.find(v => 
        v.timestamp >= phoneme.start_time && v.timestamp <= phoneme.end_time
      )
      
      if (correspondingViseme) {
        const expectedViseme = PHONEME_TO_VISEME_MAP[phoneme.phoneme]
        if (expectedViseme === correspondingViseme.viseme) {
          alignedCount++
        }
      }
    }
    
    return alignedCount / phonemes.length
  }

  private calculateTemporalConsistency(visemes: VisemeFrame[]): number {
    if (visemes.length < 2) return 1
    
    let consistencyScore = 0
    
    for (let i = 1; i < visemes.length; i++) {
      const prev = visemes[i - 1]
      const current = visemes[i]
      
      // Verificar se a transi√ß√£o √© suave
      const intensityDiff = Math.abs(current.intensity - prev.intensity)
      const timeDiff = current.timestamp - prev.timestamp
      
      // Penalizar mudan√ßas muito bruscas
      const transitionSmoothness = Math.max(0, 1 - (intensityDiff / timeDiff) * 10)
      consistencyScore += transitionSmoothness
    }
    
    return consistencyScore / (visemes.length - 1)
  }

  // =====================================================
  // M√âTODOS AUXILIARES
  // =====================================================

  private generateJobId(): string {
    return `lipsync_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
  }

  // =====================================================
  // M√âTODOS P√öBLICOS DE UTILIDADE
  // =====================================================

  getVisemeTypes(): VisemeType[] {
    return Object.values(VisemeType)
  }

  getBlendShapeTargets(): BlendShapeTarget[] {
    return Object.values(BlendShapeTarget)
  }

  getEmotionTypes(): EmotionType[] {
    return Object.values(EmotionType)
  }

  getPhonemeToVisemeMap(): Record<string, VisemeType> {
    return PHONEME_TO_VISEME_MAP
  }

  getDefaultConfig(): Required<LipSyncConfig> {
    return DEFAULT_CONFIG
  }
}

export default AdvancedLipSyncProcessor
